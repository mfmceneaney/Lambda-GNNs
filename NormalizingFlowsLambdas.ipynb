{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f295f21-2b8b-45cc-807a-5b8317a13078",
   "metadata": {},
   "source": [
    "# Lambda Classification via Normalizing flows\n",
    "## Created by Rowan Kelleher, using Matthew McEneaney's Graph Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e98a1-f4ae-4d02-a2e1-d50dcdbde59b",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "Below are the library imports I use for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3423440-b266-4372-b031-e048056cc712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "#normflows implements normalizing flows with pytorch neural networks\n",
    "import normflows as nf\n",
    "from normflows import flows\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tdata\n",
    "import torch.optim as optim\n",
    "\n",
    "#dgl is used for GNN implementation\n",
    "import dgl #NOTE: for dgl.batch and dgl.unbatch\n",
    "from dgl import save_graphs, load_graphs\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl.data.utils import save_info, load_info, Subset\n",
    "\n",
    "#tqdm progress bar for monitoring training/testing time length\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#fit functions and optimization\n",
    "import scipy.optimize as opt\n",
    "from scipy.stats import crystalball\n",
    "\n",
    "#roc-curve calculations\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b6d1fb-7fdc-4d09-a8a4-4d81c4b9f95b",
   "metadata": {},
   "source": [
    "**Here we have custom functions defines either in models.py, utils.py (GNN) or NF_utils.py (NF)**\n",
    "\n",
    "utils.py contains classes and functions implementing GNNs for Lambda signal extraction; models.py contains GNN models\n",
    "\n",
    "NF_Utils.py contains several classes and functions that provide an implementation of the normflows networks for latent space domain adaptation (training, transforming, data structures, etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4c7f45-97b3-4cfb-a6b5-d917770f97d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/vossenlab/rck32/miniconda3/envs/venv/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/hpc/group/vossenlab/rck32/miniconda3/envs/venv/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/hpc/group/vossenlab/rck32/miniconda3/envs/venv/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/hpc/group/vossenlab/rck32/miniconda3/envs/venv/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "2023-12-22 22:44:15.058195: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-22 22:44:37.362652: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "#custom imports\n",
    "from utils import load_graph_dataset, train, evaluate, GraphDataset, get_graph_dataset_info, roc_curve\n",
    "from models import GIN, HeteroGIN\n",
    "from NF_utils import Latent_data, get_masked_affine, transform, train,plot_loss, test,plot_9_histos, plot_UMAP_sidebyside,plot_UMAP_overlay, create_latent_data, NFClassifier,train_classifier,test_classifier_MC,test_classifier_data,plot_classified,get_classification_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f1341-7ba4-46b0-86f5-018bd0bf00cb",
   "metadata": {},
   "source": [
    "**For classifiying lambdas, we use graph representations that have been pre-processed by Matthew using his GNN implementation at https://github.com/mfmceneaney/Lambda-GNNs**\n",
    "\n",
    "These graphs are loaded via the *GraphDataSet* constructor and the *load()* function\n",
    "\n",
    "The GNN is created with the *GIN* constructor and loaded with weights pre-trained using the *Lambda-GNNs* repo and the *load_state_dict()* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9de4853-6f7b-4236-934b-ba37c9fe20e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''                                              '''\n",
    "'''     SETTING UP LATENT SPACE REPRESENTATION   '''\n",
    "'''                                              '''\n",
    "\n",
    "#Number of graphs in each\n",
    "# DATA_max_events = 149090\n",
    "DATA_max_events = 249090\n",
    "MC_max_events = 141118\n",
    "\n",
    "# Data and MC both have the same prefix\n",
    "prefix = \"/hpc/group/vossenlab/mfm45/.dgl/\"\n",
    "\n",
    "# MC inside Lambda_train_matched_jobs_outbending_cache_bg50nA_7_28_22__pT_phi_theta_beta_chi2_pid_status__Normalized\n",
    "MCdataset = \"Lambda_train_matched_jobs_outbending_cache_bg50nA_7_28_22__pT_phi_theta_beta_chi2_pid_status__Normalized\"\n",
    "\n",
    "# Data inside data_jobs_rga_fall2018_7_28_22__pT_phi_theta_beta_chi2_pid_status__Normalized\n",
    "DATAdataset = \"data_jobs_rga_fall2018_7_28_22__pT_phi_theta_beta_chi2_pid_status__Normalized\"\n",
    "\n",
    "max_events = 1e5\n",
    "split = 0.1\n",
    "nlayers = 2\n",
    "nmlp = 3\n",
    "hdim = 64\n",
    "nclasses, nfeatures, nfeatures_edge = get_graph_dataset_info(dataset=MCdataset, prefix=prefix)\n",
    "dropout = 0.8\n",
    "learn_eps = False\n",
    "batch = 256\n",
    "indices = None\n",
    "nworkers = 0\n",
    "npooling = \"max\"\n",
    "gpooling = \"max\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "#select model\n",
    "extractor = GIN(nlayers, nmlp, nfeatures,\n",
    "            hdim, nclasses, dropout, learn_eps, npooling, gpooling).to(device)\n",
    "extractor.load_state_dict(torch.load(\"logs/model_weights\",map_location=device))\n",
    "#select training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6521bb3-4795-40c6-94b9-7051ff4dca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCgraph_dataset = GraphDataset(prefix+MCdataset)\n",
    "DATAgraph_dataset = GraphDataset(prefix+DATAdataset)\n",
    "MCgraph_dataset.load()\n",
    "DATAgraph_dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32f049f-481c-40bd-ae12-812d5dde4833",
   "metadata": {},
   "source": [
    "**Now we define the number of samples per batch for our NF training and create *Latent_data* data structures for training the NF models**\n",
    "\n",
    "The create_latent_data function extracts the latent representation of the data using the GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b52cd2-a32c-42ab-8617-176142aaea48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No mode given, defaulting to training\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/vossenlab/rck32/miniconda3/envs/venv/lib/python3.11/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No mode given, defaulting to training\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_samples = 100\n",
    "\n",
    "training_data_DATA = create_latent_data(DATAdataset, extractor,num_samples = num_samples, max_events = DATA_max_events,  given_dataset = True, in_dataset = DATAgraph_dataset)\n",
    "training_data_MC = create_latent_data(MCdataset, extractor,num_samples = num_samples, max_events = MC_max_events,  given_dataset = True, in_dataset = MCgraph_dataset)\n",
    "\n",
    "testing_data_DATA = create_latent_data(DATAdataset, extractor, mode = \"test\",num_samples = num_samples, max_events = DATA_max_events, sidebands =False, given_dataset = True, in_dataset = DATAgraph_dataset)\n",
    "testing_data_MC = create_latent_data(MCdataset, extractor, mode = \"test\",num_samples = num_samples, max_events = MC_max_events,  given_dataset = True, in_dataset = MCgraph_dataset)\n",
    "\n",
    "val_data_DATA = create_latent_data(DATAdataset, extractor, mode = \"val\",num_samples = num_samples, max_events = DATA_max_events,  given_dataset = True, in_dataset = DATAgraph_dataset)\n",
    "val_data_MC = create_latent_data(MCdataset, extractor, mode = \"val\",num_samples = num_samples, max_events = MC_max_events,  given_dataset = True, in_dataset = MCgraph_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610110ca-e203-4f52-8d02-b5166468a925",
   "metadata": {},
   "source": [
    "Now we create the two NF models (Data and MC) using the *nf.NormalizingFlow()* constructor\n",
    "\n",
    "The *get_masked_affine* function that returns a list of *nf.flows.MaskedAffineFlow* from the *normflows* library which are the network layers\n",
    "\n",
    "We define the number of layers in the model as well as the base distribution used in training NFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98e51eb8-cbd8-484d-a5fc-5c9a83c38ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP DATA MODEL\n",
    "num_layers = 52\n",
    "\n",
    "masked_affine_flows_train_DATA = get_masked_affine(num_layers)\n",
    "distribution_DATA = nf.distributions.DiagGaussian(training_data_DATA.latent_size, trainable = False)\n",
    "masked_affine_model_DATA = nf.NormalizingFlow(q0=distribution_DATA, flows=masked_affine_flows_train_DATA)\n",
    "DATA_model = masked_affine_model_DATA.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1d963d4-4397-4667-b96b-ac5527b6b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP MC MODEL\n",
    "\n",
    "masked_affine_flows_train_MC = get_masked_affine(num_layers)\n",
    "distribution_MC = nf.distributions.DiagGaussian(training_data_MC.latent_size, trainable = False)\n",
    "masked_affine_model_MC = nf.NormalizingFlow(q0=distribution_MC, flows=masked_affine_flows_train_MC)\n",
    "MC_model = masked_affine_model_MC.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e58524-d3e7-471d-a4ae-d7c65c15ab77",
   "metadata": {},
   "source": [
    "Now we train the MC model using the *train* function from *NF_utils.py* which returns training stats for easy plotting\n",
    "(num_epochs defines how many epochs to train, compact_num defines the number of training iterations to average over when saving loss data for plotting)\n",
    "\n",
    "The *plot_loss* function displays the loss curve for estimating performance, and prints the lowest lost achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e270e1e4-478d-4a9e-b46d-41d4b470f296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1128/1128 [02:48<00:00,  6.71it/s]\n",
      "100%|██████████| 1128/1128 [02:47<00:00,  6.74it/s]\n",
      "100%|██████████| 1128/1128 [02:27<00:00,  7.67it/s]\n",
      "100%|██████████| 1128/1128 [02:39<00:00,  7.07it/s]\n",
      "100%|██████████| 1128/1128 [02:45<00:00,  6.83it/s]\n",
      "100%|██████████| 1128/1128 [02:42<00:00,  6.95it/s]\n",
      "100%|██████████| 1128/1128 [02:54<00:00,  6.46it/s]\n",
      "100%|██████████| 1128/1128 [02:41<00:00,  6.98it/s]\n",
      "100%|██████████| 1128/1128 [02:34<00:00,  7.32it/s]\n",
      "100%|██████████| 1128/1128 [02:34<00:00,  7.31it/s]\n",
      "100%|██████████| 1128/1128 [02:39<00:00,  7.05it/s]\n"
     ]
    },
    {
     "data": {
      "application/pdf": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600.504688pt\" height=\"585.478125pt\" viewBox=\"0 0 600.504688 585.478125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-12-22T23:17:54.897605</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 585.478125 \n",
       "L 600.504688 585.478125 \n",
       "L 600.504688 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 35.304688 561.6 \n",
       "L 593.304688 561.6 \n",
       "L 593.304688 7.2 \n",
       "L 35.304688 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mc2f26a2c0c\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc2f26a2c0c\" x=\"60.668324\" y=\"561.6\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(57.487074 576.198438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc2f26a2c0c\" x=\"142.618684\" y=\"561.6\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(133.074934 576.198438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc2f26a2c0c\" x=\"224.569043\" y=\"561.6\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(215.025293 576.198438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc2f26a2c0c\" x=\"306.519403\" y=\"561.6\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 300 -->\n",
       "      <g transform=\"translate(296.975653 576.198438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc2f26a2c0c\" x=\"388.469763\" y=\"561.6\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(378.926013 576.198438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc2f26a2c0c\" x=\"470.420123\" y=\"561.6\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 500 -->\n",
       "      <g transform=\"translate(460.876373 576.198438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc2f26a2c0c\" x=\"552.370483\" y=\"561.6\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 600 -->\n",
       "      <g transform=\"translate(542.826733 576.198438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <defs>\n",
       "       <path id=\"ma1723e2b48\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma1723e2b48\" x=\"35.304688\" y=\"493.901091\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- −50 -->\n",
       "      <g transform=\"translate(7.2 497.70031) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"147.412109\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma1723e2b48\" x=\"35.304688\" y=\"402.355848\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(21.942187 406.155066) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma1723e2b48\" x=\"35.304688\" y=\"310.810604\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(15.579687 314.609823) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma1723e2b48\" x=\"35.304688\" y=\"219.265361\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(9.217187 223.064579) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma1723e2b48\" x=\"35.304688\" y=\"127.720117\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(9.217187 131.519336) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#ma1723e2b48\" x=\"35.304688\" y=\"36.174874\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(9.217187 39.974093) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_14\">\n",
       "    <path d=\"M 60.668324 187.948379 \n",
       "L 61.487827 300.913596 \n",
       "L 62.307331 341.294668 \n",
       "L 63.126835 362.006601 \n",
       "L 63.946338 373.548242 \n",
       "L 65.585345 390.35991 \n",
       "L 66.404849 389.374448 \n",
       "L 67.224353 397.91687 \n",
       "L 68.043856 400.736858 \n",
       "L 68.86336 405.011795 \n",
       "L 69.682863 404.822065 \n",
       "L 71.321871 415.096038 \n",
       "L 72.141374 418.330611 \n",
       "L 72.960878 417.371093 \n",
       "L 73.780381 420.487236 \n",
       "L 74.599885 424.355945 \n",
       "L 75.419389 420.687256 \n",
       "L 76.238892 422.62814 \n",
       "L 77.058396 426.576023 \n",
       "L 77.877899 428.74676 \n",
       "L 78.697403 429.988781 \n",
       "L 79.516907 429.884911 \n",
       "L 80.33641 431.658765 \n",
       "L 81.155914 431.855963 \n",
       "L 81.975417 435.675245 \n",
       "L 82.794921 437.617362 \n",
       "L 83.614425 433.897577 \n",
       "L 84.433928 435.992531 \n",
       "L 85.253432 440.772509 \n",
       "L 86.072935 441.225838 \n",
       "L 86.892439 439.428589 \n",
       "L 87.711943 440.196629 \n",
       "L 88.531446 440.466088 \n",
       "L 89.35095 439.774265 \n",
       "L 90.170453 440.412247 \n",
       "L 90.989957 445.265191 \n",
       "L 91.809461 442.190433 \n",
       "L 92.628964 447.664661 \n",
       "L 93.448468 441.095995 \n",
       "L 94.267971 446.320088 \n",
       "L 95.087475 445.496005 \n",
       "L 95.906979 449.664321 \n",
       "L 96.726482 451.142855 \n",
       "L 97.545986 449.177766 \n",
       "L 98.365489 449.47793 \n",
       "L 99.184993 447.831889 \n",
       "L 100.004497 451.425437 \n",
       "L 100.824 453.521312 \n",
       "L 101.643504 453.775586 \n",
       "L 102.463007 454.481774 \n",
       "L 103.282511 454.409016 \n",
       "L 104.102015 453.914588 \n",
       "L 104.921518 454.043264 \n",
       "L 106.560525 456.605651 \n",
       "L 107.380029 456.926629 \n",
       "L 109.019036 458.894232 \n",
       "L 109.83854 458.653632 \n",
       "L 110.658043 457.214499 \n",
       "L 111.477547 459.397909 \n",
       "L 112.297051 458.477658 \n",
       "L 113.116554 462.360709 \n",
       "L 113.936058 458.997393 \n",
       "L 114.755561 459.343565 \n",
       "L 115.575065 461.492805 \n",
       "L 116.394569 459.397649 \n",
       "L 117.214072 462.149309 \n",
       "L 118.033576 462.932553 \n",
       "L 118.853079 462.953829 \n",
       "L 119.672583 459.772143 \n",
       "L 120.492087 462.289532 \n",
       "L 121.31159 468.573247 \n",
       "L 122.131094 461.782631 \n",
       "L 122.950597 465.461529 \n",
       "L 123.770101 463.422848 \n",
       "L 124.589605 463.734788 \n",
       "L 125.409108 465.947589 \n",
       "L 126.228612 466.644722 \n",
       "L 127.048115 466.677532 \n",
       "L 127.867619 469.606621 \n",
       "L 128.687123 466.216506 \n",
       "L 129.506626 466.403722 \n",
       "L 130.32613 465.836341 \n",
       "L 131.145633 465.588694 \n",
       "L 131.965137 469.924434 \n",
       "L 132.784641 467.966689 \n",
       "L 133.604144 469.230855 \n",
       "L 134.423648 469.989615 \n",
       "L 135.243151 469.66764 \n",
       "L 136.062655 469.487541 \n",
       "L 136.882158 472.592 \n",
       "L 137.701662 469.88549 \n",
       "L 138.521166 472.553796 \n",
       "L 139.340669 469.634569 \n",
       "L 140.160173 474.213065 \n",
       "L 140.979676 472.619637 \n",
       "L 141.79918 470.528216 \n",
       "L 142.618684 472.576989 \n",
       "L 143.438187 472.811372 \n",
       "L 144.257691 473.172938 \n",
       "L 145.077194 469.497653 \n",
       "L 145.896698 472.703471 \n",
       "L 146.716202 472.487595 \n",
       "L 147.535705 474.854699 \n",
       "L 148.355209 476.155415 \n",
       "L 149.174712 476.959794 \n",
       "L 149.994216 474.888089 \n",
       "L 150.81372 473.943692 \n",
       "L 151.633223 477.697504 \n",
       "L 152.452727 477.313631 \n",
       "L 154.091734 477.016649 \n",
       "L 154.911238 479.096309 \n",
       "L 155.730741 475.158576 \n",
       "L 156.550245 476.892423 \n",
       "L 157.369748 480.642435 \n",
       "L 158.189252 477.224206 \n",
       "L 159.008756 479.546889 \n",
       "L 159.828259 478.217414 \n",
       "L 160.647763 478.08126 \n",
       "L 161.467266 478.712798 \n",
       "L 163.106274 478.104851 \n",
       "L 163.925777 480.444123 \n",
       "L 165.564784 477.463883 \n",
       "L 166.384288 481.859114 \n",
       "L 167.203792 483.171318 \n",
       "L 168.023295 482.157058 \n",
       "L 168.842799 482.090503 \n",
       "L 169.662302 478.619719 \n",
       "L 170.481806 481.152179 \n",
       "L 171.30131 481.296898 \n",
       "L 172.120813 483.015897 \n",
       "L 172.940317 482.630598 \n",
       "L 173.75982 483.272338 \n",
       "L 174.579324 484.883279 \n",
       "L 175.398828 481.612944 \n",
       "L 176.218331 482.440624 \n",
       "L 177.037835 480.652363 \n",
       "L 177.857338 482.678595 \n",
       "L 178.676842 485.485798 \n",
       "L 179.496346 479.442633 \n",
       "L 180.315849 486.19648 \n",
       "L 181.135353 485.599138 \n",
       "L 181.954856 483.568702 \n",
       "L 182.77436 486.634738 \n",
       "L 183.593864 488.501047 \n",
       "L 184.413367 486.011824 \n",
       "L 185.232871 486.750879 \n",
       "L 186.052374 484.980827 \n",
       "L 186.871878 485.667874 \n",
       "L 187.691382 484.720967 \n",
       "L 188.510885 488.646653 \n",
       "L 190.149892 485.669799 \n",
       "L 190.969396 485.295163 \n",
       "L 191.7889 486.031492 \n",
       "L 192.608403 481.540006 \n",
       "L 194.24741 485.428165 \n",
       "L 195.066914 486.886802 \n",
       "L 195.886418 488.594431 \n",
       "L 196.705921 485.934278 \n",
       "L 197.525425 482.632409 \n",
       "L 198.344928 488.564208 \n",
       "L 199.164432 489.365557 \n",
       "L 200.803439 491.739638 \n",
       "L 201.622943 488.212109 \n",
       "L 202.442446 490.580532 \n",
       "L 203.26195 491.119529 \n",
       "L 204.081454 490.943983 \n",
       "L 204.900957 489.972396 \n",
       "L 205.720461 492.101228 \n",
       "L 206.539964 487.240307 \n",
       "L 207.359468 490.343035 \n",
       "L 208.178972 487.362882 \n",
       "L 208.998475 487.073606 \n",
       "L 209.817979 490.210455 \n",
       "L 210.637482 490.654679 \n",
       "L 211.456986 490.766299 \n",
       "L 212.27649 493.260877 \n",
       "L 213.095993 489.401681 \n",
       "L 213.915497 494.822956 \n",
       "L 214.735 491.8069 \n",
       "L 215.554504 490.784245 \n",
       "L 216.374008 493.258398 \n",
       "L 217.193511 488.472823 \n",
       "L 218.013015 491.84708 \n",
       "L 218.832518 490.877405 \n",
       "L 219.652022 494.664399 \n",
       "L 220.471526 495.898446 \n",
       "L 221.291029 491.957868 \n",
       "L 222.110533 492.782605 \n",
       "L 222.930036 490.743452 \n",
       "L 223.74954 495.716086 \n",
       "L 224.569043 495.093388 \n",
       "L 225.388547 493.986835 \n",
       "L 226.208051 494.061438 \n",
       "L 227.027554 493.493633 \n",
       "L 227.847058 494.410175 \n",
       "L 228.666561 494.020434 \n",
       "L 229.486065 496.992689 \n",
       "L 230.305569 495.782644 \n",
       "L 231.125072 493.761782 \n",
       "L 231.944576 494.005091 \n",
       "L 232.764079 497.324827 \n",
       "L 233.583583 493.240904 \n",
       "L 234.403087 500.204293 \n",
       "L 235.22259 493.775943 \n",
       "L 236.042094 491.592313 \n",
       "L 236.861597 497.805241 \n",
       "L 237.681101 496.202056 \n",
       "L 238.500605 495.601901 \n",
       "L 239.320108 496.96893 \n",
       "L 240.139612 498.031841 \n",
       "L 240.959115 496.849024 \n",
       "L 241.778619 497.202457 \n",
       "L 242.598123 496.001853 \n",
       "L 243.417626 496.684529 \n",
       "L 245.056633 496.65208 \n",
       "L 245.876137 498.936829 \n",
       "L 247.515144 499.50313 \n",
       "L 248.334648 501.788931 \n",
       "L 249.154151 497.738001 \n",
       "L 249.973655 500.50471 \n",
       "L 250.793159 499.395301 \n",
       "L 252.432166 496.171009 \n",
       "L 253.251669 497.877434 \n",
       "L 254.890677 495.182392 \n",
       "L 255.71018 495.255908 \n",
       "L 256.529684 498.672057 \n",
       "L 257.349187 500.845828 \n",
       "L 258.988195 500.380589 \n",
       "L 259.807698 501.148869 \n",
       "L 260.627202 501.505152 \n",
       "L 261.446705 500.606743 \n",
       "L 262.266209 498.606888 \n",
       "L 263.085713 500.715167 \n",
       "L 264.72472 497.812523 \n",
       "L 265.544223 501.752207 \n",
       "L 266.363727 501.462132 \n",
       "L 267.183231 501.689891 \n",
       "L 268.002734 496.811796 \n",
       "L 268.822238 500.163975 \n",
       "L 269.641741 501.807269 \n",
       "L 270.461245 501.35523 \n",
       "L 271.280749 504.590585 \n",
       "L 272.100252 501.071742 \n",
       "L 272.919756 501.343755 \n",
       "L 273.739259 492.606602 \n",
       "L 275.378267 502.948725 \n",
       "L 276.19777 505.912636 \n",
       "L 277.017274 502.674168 \n",
       "L 277.836777 502.363977 \n",
       "L 278.656281 503.521905 \n",
       "L 279.475785 502.565978 \n",
       "L 280.295288 500.62524 \n",
       "L 281.114792 501.173284 \n",
       "L 281.934295 501.546607 \n",
       "L 282.753799 502.971538 \n",
       "L 283.573303 504.631196 \n",
       "L 284.392806 500.672113 \n",
       "L 285.21231 503.103274 \n",
       "L 286.031813 503.84581 \n",
       "L 286.851317 502.81994 \n",
       "L 287.670821 505.000266 \n",
       "L 288.490324 503.866881 \n",
       "L 289.309828 503.298342 \n",
       "L 290.129331 503.918284 \n",
       "L 290.948835 504.235087 \n",
       "L 291.768339 506.248619 \n",
       "L 292.587842 506.731814 \n",
       "L 293.407346 506.257768 \n",
       "L 294.226849 505.42824 \n",
       "L 295.046353 504.132228 \n",
       "L 295.865857 506.71919 \n",
       "L 296.68536 504.697713 \n",
       "L 297.504864 507.226086 \n",
       "L 298.324367 504.22275 \n",
       "L 299.143871 503.687943 \n",
       "L 299.963375 504.084339 \n",
       "L 300.782878 507.529562 \n",
       "L 301.602382 501.607087 \n",
       "L 302.421885 506.463403 \n",
       "L 304.060893 505.586152 \n",
       "L 304.880396 506.834603 \n",
       "L 305.6999 509.557838 \n",
       "L 306.519403 507.572153 \n",
       "L 307.338907 506.202749 \n",
       "L 308.158411 506.241723 \n",
       "L 308.977914 507.695315 \n",
       "L 309.797418 504.828575 \n",
       "L 310.616921 504.270216 \n",
       "L 311.436425 507.662593 \n",
       "L 312.255929 508.320646 \n",
       "L 313.075432 511.043724 \n",
       "L 313.894936 504.349025 \n",
       "L 314.714439 505.740795 \n",
       "L 315.533943 504.117531 \n",
       "L 316.353446 508.337816 \n",
       "L 317.17295 509.936408 \n",
       "L 317.992454 508.360153 \n",
       "L 318.811957 503.716298 \n",
       "L 319.631461 507.865516 \n",
       "L 320.450964 508.600676 \n",
       "L 321.270468 508.829635 \n",
       "L 322.089972 509.227219 \n",
       "L 322.909475 508.065927 \n",
       "L 323.728979 507.911965 \n",
       "L 324.548482 509.274104 \n",
       "L 325.367986 509.525583 \n",
       "L 326.18749 504.905363 \n",
       "L 327.006993 509.885027 \n",
       "L 327.826497 512.31568 \n",
       "L 328.646 504.326674 \n",
       "L 329.465504 510.433758 \n",
       "L 330.285008 510.131581 \n",
       "L 331.104511 509.617555 \n",
       "L 331.924015 508.843644 \n",
       "L 332.743518 512.04723 \n",
       "L 333.563022 510.552173 \n",
       "L 334.382526 511.243865 \n",
       "L 335.202029 507.189299 \n",
       "L 336.021533 508.24887 \n",
       "L 336.841036 511.474203 \n",
       "L 337.66054 513.232047 \n",
       "L 338.480044 511.463036 \n",
       "L 339.299547 514.161388 \n",
       "L 340.119051 511.551216 \n",
       "L 340.938554 511.693461 \n",
       "L 341.758058 513.579555 \n",
       "L 342.577562 510.483505 \n",
       "L 343.397065 512.606055 \n",
       "L 344.216569 511.634724 \n",
       "L 345.036072 511.512966 \n",
       "L 345.855576 511.516632 \n",
       "L 346.67508 511.280888 \n",
       "L 347.494583 506.663169 \n",
       "L 348.314087 511.866048 \n",
       "L 349.13359 511.195203 \n",
       "L 349.953094 511.790279 \n",
       "L 350.772598 513.462471 \n",
       "L 351.592101 510.335572 \n",
       "L 352.411605 517.731311 \n",
       "L 353.231108 512.979742 \n",
       "L 354.050612 511.316333 \n",
       "L 354.870116 511.470033 \n",
       "L 355.689619 510.298242 \n",
       "L 356.509123 512.216381 \n",
       "L 357.328626 511.871089 \n",
       "L 358.14813 515.857742 \n",
       "L 358.967634 512.148239 \n",
       "L 359.787137 512.232352 \n",
       "L 360.606641 511.67845 \n",
       "L 361.426144 512.561488 \n",
       "L 362.245648 516.145236 \n",
       "L 363.065152 513.895218 \n",
       "L 363.884655 514.99185 \n",
       "L 364.704159 514.80458 \n",
       "L 365.523662 512.757609 \n",
       "L 366.343166 515.168295 \n",
       "L 367.16267 514.965445 \n",
       "L 367.982173 516.812247 \n",
       "L 368.801677 515.624497 \n",
       "L 369.62118 512.97046 \n",
       "L 370.440684 511.244753 \n",
       "L 371.260188 515.353324 \n",
       "L 372.079691 496.006845 \n",
       "L 372.899195 511.32153 \n",
       "L 373.718698 512.667203 \n",
       "L 374.538202 513.637359 \n",
       "L 375.357706 515.110188 \n",
       "L 376.177209 515.47429 \n",
       "L 376.996713 515.431312 \n",
       "L 377.816216 515.002202 \n",
       "L 378.63572 515.872521 \n",
       "L 379.455224 517.151055 \n",
       "L 380.274727 517.751841 \n",
       "L 381.094231 512.317674 \n",
       "L 382.733238 516.600914 \n",
       "L 383.552742 517.781767 \n",
       "L 384.372245 514.084335 \n",
       "L 385.191749 513.95139 \n",
       "L 386.011252 515.526819 \n",
       "L 386.830756 513.929417 \n",
       "L 387.65026 516.644717 \n",
       "L 388.469763 517.074619 \n",
       "L 389.289267 516.096325 \n",
       "L 390.10877 519.682482 \n",
       "L 391.747778 515.183988 \n",
       "L 393.386785 514.861047 \n",
       "L 394.206288 515.012222 \n",
       "L 395.025792 517.222561 \n",
       "L 395.845296 518.742576 \n",
       "L 396.664799 515.78947 \n",
       "L 397.484303 515.102171 \n",
       "L 398.303806 519.971221 \n",
       "L 399.12331 519.759271 \n",
       "L 400.762317 515.091039 \n",
       "L 401.581821 516.139793 \n",
       "L 402.401324 515.2719 \n",
       "L 403.220828 516.183429 \n",
       "L 404.040332 518.650848 \n",
       "L 404.859835 514.532756 \n",
       "L 405.679339 517.682768 \n",
       "L 406.498842 516.498303 \n",
       "L 407.318346 520.53068 \n",
       "L 408.137849 513.618944 \n",
       "L 408.957353 517.325608 \n",
       "L 409.776857 522.908443 \n",
       "L 410.59636 518.722492 \n",
       "L 411.415864 519.339036 \n",
       "L 412.235367 520.876626 \n",
       "L 413.054871 515.820514 \n",
       "L 413.874375 516.42504 \n",
       "L 414.693878 521.482925 \n",
       "L 415.513382 516.575584 \n",
       "L 416.332885 475.452942 \n",
       "L 417.152389 455.428447 \n",
       "L 417.971893 483.834038 \n",
       "L 419.6109 502.907077 \n",
       "L 420.430403 506.862653 \n",
       "L 421.249907 506.421907 \n",
       "L 422.069411 508.813173 \n",
       "L 422.888914 509.419763 \n",
       "L 423.708418 510.290884 \n",
       "L 424.527921 512.181984 \n",
       "L 425.347425 512.411366 \n",
       "L 426.166929 513.982327 \n",
       "L 426.986432 511.0387 \n",
       "L 427.805936 512.292038 \n",
       "L 428.625439 516.272717 \n",
       "L 430.264447 514.720514 \n",
       "L 431.08395 516.232349 \n",
       "L 431.903454 516.087577 \n",
       "L 432.722957 515.349801 \n",
       "L 433.542461 517.404532 \n",
       "L 434.361965 514.449994 \n",
       "L 435.181468 515.224268 \n",
       "L 436.000972 518.489246 \n",
       "L 436.820475 519.233702 \n",
       "L 437.639979 514.920313 \n",
       "L 438.459483 512.985035 \n",
       "L 439.278986 516.460696 \n",
       "L 440.09849 516.492903 \n",
       "L 440.917993 519.561484 \n",
       "L 442.557001 516.637146 \n",
       "L 443.376504 518.527963 \n",
       "L 444.196008 522.411573 \n",
       "L 445.015511 514.322511 \n",
       "L 445.835015 515.144579 \n",
       "L 446.654519 518.020121 \n",
       "L 447.474022 520.464055 \n",
       "L 448.293526 518.054228 \n",
       "L 449.932533 520.839388 \n",
       "L 450.752037 517.850709 \n",
       "L 451.57154 524.01657 \n",
       "L 452.391044 518.459238 \n",
       "L 453.210547 520.402371 \n",
       "L 454.030051 519.4236 \n",
       "L 454.849555 523.606226 \n",
       "L 455.669058 521.570433 \n",
       "L 456.488562 518.085866 \n",
       "L 457.308065 520.956432 \n",
       "L 458.127569 516.038343 \n",
       "L 458.947073 518.312564 \n",
       "L 459.766576 522.623108 \n",
       "L 460.58608 524.747127 \n",
       "L 461.405583 522.22039 \n",
       "L 462.225087 520.689199 \n",
       "L 463.044591 521.645607 \n",
       "L 463.864094 515.754118 \n",
       "L 464.683598 516.230455 \n",
       "L 465.503101 523.956771 \n",
       "L 466.322605 525.491529 \n",
       "L 467.142109 520.923718 \n",
       "L 467.961612 524.572448 \n",
       "L 468.781116 522.032385 \n",
       "L 469.600619 520.593972 \n",
       "L 470.420123 522.764979 \n",
       "L 471.239627 522.671473 \n",
       "L 472.05913 522.185071 \n",
       "L 472.878634 524.505785 \n",
       "L 473.698137 522.609618 \n",
       "L 474.517641 521.894592 \n",
       "L 475.337145 523.724974 \n",
       "L 476.156648 520.653432 \n",
       "L 476.976152 523.392855 \n",
       "L 477.795655 525.017874 \n",
       "L 478.615159 522.688006 \n",
       "L 479.434663 522.922168 \n",
       "L 480.254166 519.522223 \n",
       "L 481.07367 522.757132 \n",
       "L 481.893173 523.088103 \n",
       "L 482.712677 526.577333 \n",
       "L 483.532181 522.161076 \n",
       "L 484.351684 522.533208 \n",
       "L 485.171188 523.167245 \n",
       "L 485.990691 519.078365 \n",
       "L 486.810195 521.616822 \n",
       "L 487.629699 524.612912 \n",
       "L 488.449202 522.458299 \n",
       "L 489.268706 525.020594 \n",
       "L 490.088209 525.222231 \n",
       "L 490.907713 528.981531 \n",
       "L 491.727217 523.205193 \n",
       "L 492.54672 523.529179 \n",
       "L 493.366224 525.784019 \n",
       "L 494.185727 519.198835 \n",
       "L 495.005231 521.773405 \n",
       "L 496.644238 525.350926 \n",
       "L 498.283245 526.200545 \n",
       "L 499.102749 519.880544 \n",
       "L 499.922252 524.111052 \n",
       "L 500.741756 526.498345 \n",
       "L 501.56126 520.25891 \n",
       "L 502.380763 525.06694 \n",
       "L 503.200267 526.020962 \n",
       "L 504.01977 521.867235 \n",
       "L 504.839274 524.686792 \n",
       "L 505.658778 525.712302 \n",
       "L 506.478281 526.527366 \n",
       "L 507.297785 526.45758 \n",
       "L 508.117288 527.763459 \n",
       "L 508.936792 525.912026 \n",
       "L 509.756296 524.868991 \n",
       "L 510.575799 523.280018 \n",
       "L 511.395303 525.143602 \n",
       "L 512.214806 527.836421 \n",
       "L 513.03431 527.542239 \n",
       "L 513.853814 526.063659 \n",
       "L 514.673317 526.866844 \n",
       "L 516.312324 527.998899 \n",
       "L 517.131828 529.340511 \n",
       "L 517.951332 522.942402 \n",
       "L 518.770835 523.911098 \n",
       "L 519.590339 525.744759 \n",
       "L 520.409842 525.974169 \n",
       "L 521.229346 530.131917 \n",
       "L 522.04885 530.040819 \n",
       "L 522.868353 529.451594 \n",
       "L 523.687857 526.301024 \n",
       "L 524.50736 527.737882 \n",
       "L 525.326864 528.509209 \n",
       "L 526.146368 524.955174 \n",
       "L 526.965871 527.54399 \n",
       "L 527.785375 528.23871 \n",
       "L 528.604878 529.921684 \n",
       "L 529.424382 525.952275 \n",
       "L 530.243886 523.97508 \n",
       "L 531.063389 526.260399 \n",
       "L 531.882893 527.394305 \n",
       "L 532.702396 526.145088 \n",
       "L 533.5219 527.433619 \n",
       "L 534.341404 527.783389 \n",
       "L 535.980411 530.560912 \n",
       "L 536.799914 531.099674 \n",
       "L 537.619418 530.980382 \n",
       "L 538.438922 527.810733 \n",
       "L 539.258425 520.558496 \n",
       "L 540.077929 521.123303 \n",
       "L 541.716936 529.694438 \n",
       "L 542.53644 527.155282 \n",
       "L 543.355943 527.842721 \n",
       "L 544.175447 530.607446 \n",
       "L 544.99495 527.501586 \n",
       "L 545.814454 530.853537 \n",
       "L 546.633958 529.863862 \n",
       "L 547.453461 529.493755 \n",
       "L 548.272965 530.718285 \n",
       "L 549.092468 528.10358 \n",
       "L 549.911972 529.841054 \n",
       "L 550.731476 533.30709 \n",
       "L 551.550979 522.906203 \n",
       "L 552.370483 529.431868 \n",
       "L 553.189986 532.743303 \n",
       "L 554.00949 531.362456 \n",
       "L 554.828994 524.545814 \n",
       "L 555.648497 526.920806 \n",
       "L 556.468001 527.442446 \n",
       "L 557.287504 529.130253 \n",
       "L 558.107008 531.953674 \n",
       "L 558.926512 531.003806 \n",
       "L 559.746015 528.63449 \n",
       "L 560.565519 524.966196 \n",
       "L 561.385022 524.892545 \n",
       "L 562.204526 529.24093 \n",
       "L 563.02403 531.784659 \n",
       "L 563.843533 532.315507 \n",
       "L 564.663037 530.636762 \n",
       "L 565.48254 533.028529 \n",
       "L 566.302044 531.123414 \n",
       "L 567.121548 528.898351 \n",
       "L 567.121548 528.898351 \n",
       "\" clip-path=\"url(#p49ad460a07)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 2; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_15\">\n",
       "    <path d=\"M 60.668324 32.4 \n",
       "L 61.487827 277.316201 \n",
       "L 62.307331 333.256851 \n",
       "L 63.126835 357.501904 \n",
       "L 63.946338 367.391944 \n",
       "L 64.765842 382.698698 \n",
       "L 65.585345 388.001389 \n",
       "L 67.224353 392.954466 \n",
       "L 68.043856 401.608956 \n",
       "L 68.86336 407.2195 \n",
       "L 69.682863 407.533853 \n",
       "L 70.502367 408.185267 \n",
       "L 71.321871 412.204165 \n",
       "L 72.141374 420.130169 \n",
       "L 72.960878 416.181557 \n",
       "L 73.780381 423.827099 \n",
       "L 74.599885 421.3605 \n",
       "L 75.419389 422.192586 \n",
       "L 77.058396 426.231672 \n",
       "L 77.877899 430.112934 \n",
       "L 78.697403 429.788271 \n",
       "L 80.33641 431.900187 \n",
       "L 81.155914 430.763808 \n",
       "L 81.975417 432.508164 \n",
       "L 82.794921 437.507031 \n",
       "L 83.614425 438.648556 \n",
       "L 84.433928 436.464801 \n",
       "L 85.253432 440.706957 \n",
       "L 86.072935 436.755894 \n",
       "L 86.892439 440.601999 \n",
       "L 87.711943 438.41115 \n",
       "L 89.35095 440.505887 \n",
       "L 90.170453 442.925848 \n",
       "L 90.989957 442.066405 \n",
       "L 91.809461 443.331572 \n",
       "L 92.628964 445.578901 \n",
       "L 93.448468 441.682492 \n",
       "L 95.087475 446.161858 \n",
       "L 95.906979 450.78977 \n",
       "L 96.726482 447.277257 \n",
       "L 97.545986 447.606862 \n",
       "L 98.365489 453.247759 \n",
       "L 99.184993 444.958784 \n",
       "L 100.004497 455.08378 \n",
       "L 100.824 454.670228 \n",
       "L 101.643504 454.599398 \n",
       "L 102.463007 455.376652 \n",
       "L 103.282511 453.731502 \n",
       "L 104.102015 456.545935 \n",
       "L 104.921518 456.960525 \n",
       "L 105.741022 453.830219 \n",
       "L 106.560525 460.012302 \n",
       "L 107.380029 454.043675 \n",
       "L 108.199533 453.507563 \n",
       "L 109.019036 461.38205 \n",
       "L 109.83854 462.272344 \n",
       "L 110.658043 459.762637 \n",
       "L 111.477547 459.694452 \n",
       "L 112.297051 462.574978 \n",
       "L 113.116554 463.221571 \n",
       "L 113.936058 462.400088 \n",
       "L 114.755561 460.495255 \n",
       "L 115.575065 466.05734 \n",
       "L 116.394569 466.033684 \n",
       "L 117.214072 461.969519 \n",
       "L 118.033576 463.15096 \n",
       "L 119.672583 466.584481 \n",
       "L 120.492087 463.634963 \n",
       "L 121.31159 462.246055 \n",
       "L 122.131094 464.998094 \n",
       "L 122.950597 469.279864 \n",
       "L 123.770101 462.336429 \n",
       "L 124.589605 465.743485 \n",
       "L 125.409108 462.974897 \n",
       "L 126.228612 470.250819 \n",
       "L 127.048115 468.881351 \n",
       "L 127.867619 469.026129 \n",
       "L 129.506626 465.488732 \n",
       "L 130.32613 471.423403 \n",
       "L 131.145633 464.461308 \n",
       "L 131.965137 461.956972 \n",
       "L 132.784641 468.636965 \n",
       "L 133.604144 469.459155 \n",
       "L 134.423648 468.163191 \n",
       "L 135.243151 471.584508 \n",
       "L 136.062655 469.259078 \n",
       "L 136.882158 471.009158 \n",
       "L 137.701662 468.979343 \n",
       "L 138.521166 473.403374 \n",
       "L 139.340669 466.644524 \n",
       "L 140.160173 473.347122 \n",
       "L 140.979676 470.878232 \n",
       "L 141.79918 472.542683 \n",
       "L 142.618684 472.085765 \n",
       "L 143.438187 468.410377 \n",
       "L 144.257691 475.704098 \n",
       "L 145.077194 468.815569 \n",
       "L 145.896698 471.809656 \n",
       "L 146.716202 475.966294 \n",
       "L 147.535705 474.987047 \n",
       "L 148.355209 473.66988 \n",
       "L 149.174712 477.335579 \n",
       "L 149.994216 476.786322 \n",
       "L 150.81372 477.074138 \n",
       "L 151.633223 475.471649 \n",
       "L 152.452727 482.356532 \n",
       "L 153.27223 475.710908 \n",
       "L 154.091734 470.678925 \n",
       "L 154.911238 479.04817 \n",
       "L 155.730741 480.062754 \n",
       "L 156.550245 476.523399 \n",
       "L 157.369748 482.889178 \n",
       "L 158.189252 483.166915 \n",
       "L 159.008756 478.480593 \n",
       "L 159.828259 478.645483 \n",
       "L 160.647763 480.658834 \n",
       "L 161.467266 484.519464 \n",
       "L 162.28677 486.15178 \n",
       "L 163.106274 479.038907 \n",
       "L 163.925777 481.600812 \n",
       "L 164.745281 482.688565 \n",
       "L 165.564784 478.860265 \n",
       "L 166.384288 485.97212 \n",
       "L 167.203792 480.818352 \n",
       "L 168.023295 481.724345 \n",
       "L 168.842799 484.475175 \n",
       "L 169.662302 479.260667 \n",
       "L 170.481806 479.491248 \n",
       "L 171.30131 481.419632 \n",
       "L 172.120813 485.025717 \n",
       "L 172.940317 484.673847 \n",
       "L 173.75982 481.764168 \n",
       "L 174.579324 481.014459 \n",
       "L 175.398828 481.508282 \n",
       "L 176.218331 487.657075 \n",
       "L 177.037835 480.703765 \n",
       "L 177.857338 476.813145 \n",
       "L 178.676842 483.315746 \n",
       "L 179.496346 482.605538 \n",
       "L 180.315849 482.331847 \n",
       "L 181.135353 485.477609 \n",
       "L 181.954856 483.886243 \n",
       "L 182.77436 487.871775 \n",
       "L 183.593864 481.946252 \n",
       "L 184.413367 486.399277 \n",
       "L 185.232871 486.354994 \n",
       "L 186.052374 484.419509 \n",
       "L 186.871878 481.91736 \n",
       "L 188.510885 487.900594 \n",
       "L 189.330389 482.374391 \n",
       "L 190.149892 484.193168 \n",
       "L 190.969396 489.340583 \n",
       "L 191.7889 481.59899 \n",
       "L 192.608403 485.603793 \n",
       "L 193.427907 482.166831 \n",
       "L 194.24741 487.160197 \n",
       "L 195.066914 485.291561 \n",
       "L 195.886418 486.736891 \n",
       "L 196.705921 492.340456 \n",
       "L 197.525425 482.127896 \n",
       "L 198.344928 487.975485 \n",
       "L 199.164432 490.245787 \n",
       "L 199.983936 484.92185 \n",
       "L 200.803439 488.728193 \n",
       "L 201.622943 489.639939 \n",
       "L 202.442446 492.060316 \n",
       "L 203.26195 492.383368 \n",
       "L 204.081454 495.173566 \n",
       "L 204.900957 492.814803 \n",
       "L 205.720461 495.215833 \n",
       "L 206.539964 487.700904 \n",
       "L 207.359468 494.184474 \n",
       "L 208.178972 496.588475 \n",
       "L 208.998475 490.248823 \n",
       "L 209.817979 489.911776 \n",
       "L 210.637482 492.730819 \n",
       "L 211.456986 491.742415 \n",
       "L 212.27649 496.399899 \n",
       "L 213.095993 491.221765 \n",
       "L 213.915497 489.99886 \n",
       "L 214.735 497.295736 \n",
       "L 215.554504 489.985768 \n",
       "L 216.374008 492.938837 \n",
       "L 217.193511 490.979847 \n",
       "L 218.013015 490.341495 \n",
       "L 218.832518 494.145682 \n",
       "L 219.652022 492.232191 \n",
       "L 220.471526 492.918114 \n",
       "L 221.291029 491.628739 \n",
       "L 222.110533 493.027209 \n",
       "L 222.930036 495.643084 \n",
       "L 223.74954 494.094015 \n",
       "L 224.569043 486.574039 \n",
       "L 225.388547 499.163407 \n",
       "L 226.208051 491.785026 \n",
       "L 227.027554 491.952962 \n",
       "L 227.847058 494.4961 \n",
       "L 228.666561 493.641244 \n",
       "L 229.486065 494.561031 \n",
       "L 230.305569 494.984819 \n",
       "L 231.125072 490.138756 \n",
       "L 231.944576 492.461279 \n",
       "L 232.764079 493.891577 \n",
       "L 233.583583 492.735297 \n",
       "L 234.403087 499.548741 \n",
       "L 235.22259 490.547038 \n",
       "L 236.042094 489.222121 \n",
       "L 236.861597 495.681663 \n",
       "L 237.681101 494.619803 \n",
       "L 238.500605 496.437293 \n",
       "L 239.320108 499.556728 \n",
       "L 240.139612 499.657834 \n",
       "L 240.959115 494.662812 \n",
       "L 241.778619 496.07285 \n",
       "L 242.598123 495.976653 \n",
       "L 243.417626 500.59142 \n",
       "L 244.23713 495.040026 \n",
       "L 245.056633 497.312716 \n",
       "L 245.876137 493.919062 \n",
       "L 246.695641 494.682923 \n",
       "L 247.515144 501.746242 \n",
       "L 248.334648 503.097811 \n",
       "L 249.154151 501.15044 \n",
       "L 249.973655 499.558273 \n",
       "L 250.793159 504.093685 \n",
       "L 251.612662 501.499007 \n",
       "L 252.432166 493.786893 \n",
       "L 253.251669 500.91284 \n",
       "L 254.890677 505.869874 \n",
       "L 255.71018 492.141903 \n",
       "L 256.529684 500.616849 \n",
       "L 257.349187 498.704709 \n",
       "L 258.168691 508.203642 \n",
       "L 258.988195 500.996777 \n",
       "L 259.807698 497.181372 \n",
       "L 260.627202 500.912076 \n",
       "L 261.446705 502.476331 \n",
       "L 262.266209 496.458747 \n",
       "L 263.085713 502.60589 \n",
       "L 263.905216 499.740378 \n",
       "L 264.72472 500.414637 \n",
       "L 265.544223 501.519416 \n",
       "L 266.363727 497.962055 \n",
       "L 267.183231 499.957525 \n",
       "L 268.002734 498.019288 \n",
       "L 268.822238 503.811168 \n",
       "L 269.641741 500.777329 \n",
       "L 270.461245 494.547013 \n",
       "L 271.280749 501.494474 \n",
       "L 272.100252 502.942995 \n",
       "L 272.919756 499.432673 \n",
       "L 273.739259 494.338735 \n",
       "L 274.558763 494.772992 \n",
       "L 275.378267 501.932989 \n",
       "L 276.19777 500.83516 \n",
       "L 277.017274 501.672791 \n",
       "L 277.836777 502.153062 \n",
       "L 278.656281 500.711451 \n",
       "L 279.475785 498.490794 \n",
       "L 280.295288 498.444952 \n",
       "L 281.114792 502.881632 \n",
       "L 281.934295 493.69814 \n",
       "L 282.753799 503.243479 \n",
       "L 283.573303 505.101437 \n",
       "L 284.392806 500.052513 \n",
       "L 285.21231 502.022971 \n",
       "L 286.031813 504.284381 \n",
       "L 286.851317 502.095437 \n",
       "L 287.670821 501.961122 \n",
       "L 288.490324 502.847027 \n",
       "L 289.309828 510.659044 \n",
       "L 290.129331 502.654291 \n",
       "L 290.948835 503.951004 \n",
       "L 291.768339 506.984165 \n",
       "L 292.587842 497.169882 \n",
       "L 293.407346 502.24015 \n",
       "L 294.226849 510.450836 \n",
       "L 295.046353 504.550523 \n",
       "L 295.865857 511.508217 \n",
       "L 296.68536 505.211617 \n",
       "L 297.504864 510.657615 \n",
       "L 298.324367 504.773241 \n",
       "L 299.143871 505.357263 \n",
       "L 299.963375 507.642612 \n",
       "L 300.782878 515.864635 \n",
       "L 301.602382 505.007699 \n",
       "L 302.421885 503.850795 \n",
       "L 303.241389 508.141545 \n",
       "L 304.060893 506.254987 \n",
       "L 304.880396 509.515754 \n",
       "L 305.6999 507.491688 \n",
       "L 306.519403 508.051426 \n",
       "L 307.338907 507.755723 \n",
       "L 308.158411 502.440797 \n",
       "L 308.977914 511.160156 \n",
       "L 309.797418 502.362857 \n",
       "L 310.616921 509.185165 \n",
       "L 311.436425 506.748059 \n",
       "L 312.255929 506.814961 \n",
       "L 313.075432 504.254735 \n",
       "L 313.894936 503.210803 \n",
       "L 314.714439 509.677851 \n",
       "L 315.533943 505.900713 \n",
       "L 316.353446 506.348165 \n",
       "L 317.17295 502.369285 \n",
       "L 317.992454 509.597467 \n",
       "L 318.811957 500.47652 \n",
       "L 319.631461 505.711086 \n",
       "L 320.450964 505.874251 \n",
       "L 321.270468 512.231477 \n",
       "L 322.089972 501.791312 \n",
       "L 322.909475 510.65655 \n",
       "L 323.728979 503.982444 \n",
       "L 324.548482 504.231094 \n",
       "L 325.367986 504.680751 \n",
       "L 326.18749 505.945967 \n",
       "L 327.006993 511.424376 \n",
       "L 327.826497 503.567982 \n",
       "L 328.646 501.345798 \n",
       "L 329.465504 511.507516 \n",
       "L 330.285008 504.604652 \n",
       "L 331.104511 514.028811 \n",
       "L 331.924015 506.219172 \n",
       "L 332.743518 511.975823 \n",
       "L 333.563022 512.064317 \n",
       "L 334.382526 508.534835 \n",
       "L 335.202029 511.948408 \n",
       "L 336.021533 508.098466 \n",
       "L 336.841036 508.653565 \n",
       "L 337.66054 515.901563 \n",
       "L 338.480044 504.713819 \n",
       "L 339.299547 507.549593 \n",
       "L 340.119051 512.241989 \n",
       "L 340.938554 514.776123 \n",
       "L 341.758058 513.502835 \n",
       "L 342.577562 511.536554 \n",
       "L 343.397065 515.042909 \n",
       "L 344.216569 513.703742 \n",
       "L 345.036072 512.688758 \n",
       "L 345.855576 514.61469 \n",
       "L 346.67508 517.165818 \n",
       "L 347.494583 517.202233 \n",
       "L 348.314087 508.482421 \n",
       "L 349.13359 511.330714 \n",
       "L 349.953094 511.791182 \n",
       "L 350.772598 518.161254 \n",
       "L 351.592101 513.944434 \n",
       "L 352.411605 506.258997 \n",
       "L 353.231108 514.599173 \n",
       "L 354.050612 513.701319 \n",
       "L 354.870116 507.574508 \n",
       "L 355.689619 514.070091 \n",
       "L 356.509123 506.027876 \n",
       "L 357.328626 516.48418 \n",
       "L 358.14813 514.599176 \n",
       "L 358.967634 510.339151 \n",
       "L 359.787137 509.797519 \n",
       "L 360.606641 510.094112 \n",
       "L 361.426144 516.122617 \n",
       "L 362.245648 512.961095 \n",
       "L 363.065152 503.721548 \n",
       "L 363.884655 515.747236 \n",
       "L 364.704159 515.334264 \n",
       "L 365.523662 508.863758 \n",
       "L 366.343166 515.395994 \n",
       "L 367.16267 512.290513 \n",
       "L 367.982173 513.129307 \n",
       "L 368.801677 511.277924 \n",
       "L 369.62118 513.447736 \n",
       "L 370.440684 506.95706 \n",
       "L 371.260188 511.725496 \n",
       "L 372.079691 496.020924 \n",
       "L 372.899195 508.436571 \n",
       "L 373.718698 510.786224 \n",
       "L 374.538202 506.762093 \n",
       "L 375.357706 517.058876 \n",
       "L 376.177209 511.140936 \n",
       "L 376.996713 514.273509 \n",
       "L 377.816216 516.154301 \n",
       "L 378.63572 515.298878 \n",
       "L 379.455224 514.647803 \n",
       "L 380.274727 515.980331 \n",
       "L 381.094231 513.714239 \n",
       "L 381.913734 516.72499 \n",
       "L 382.733238 513.157437 \n",
       "L 383.552742 519.10561 \n",
       "L 384.372245 514.828553 \n",
       "L 385.191749 503.381831 \n",
       "L 386.011252 514.140327 \n",
       "L 386.830756 517.832068 \n",
       "L 387.65026 514.01112 \n",
       "L 388.469763 519.59524 \n",
       "L 389.289267 520.559061 \n",
       "L 390.10877 519.3118 \n",
       "L 390.928274 517.108 \n",
       "L 391.747778 517.515533 \n",
       "L 392.567281 520.535384 \n",
       "L 393.386785 524.949949 \n",
       "L 394.206288 515.313656 \n",
       "L 395.025792 515.874366 \n",
       "L 395.845296 517.212329 \n",
       "L 396.664799 520.917377 \n",
       "L 397.484303 517.557397 \n",
       "L 398.303806 515.634239 \n",
       "L 399.12331 516.462918 \n",
       "L 399.942814 519.196456 \n",
       "L 400.762317 514.913395 \n",
       "L 401.581821 514.905479 \n",
       "L 402.401324 513.98424 \n",
       "L 403.220828 517.712783 \n",
       "L 404.040332 518.388768 \n",
       "L 404.859835 512.77322 \n",
       "L 405.679339 509.154149 \n",
       "L 406.498842 515.06755 \n",
       "L 407.318346 522.648302 \n",
       "L 408.957353 508.635492 \n",
       "L 409.776857 518.251878 \n",
       "L 410.59636 521.234304 \n",
       "L 411.415864 515.265682 \n",
       "L 412.235367 519.756847 \n",
       "L 413.054871 514.692293 \n",
       "L 413.874375 516.458202 \n",
       "L 414.693878 509.847612 \n",
       "L 415.513382 518.358445 \n",
       "L 416.332885 517.168126 \n",
       "L 417.152389 435.062237 \n",
       "L 417.971893 470.909737 \n",
       "L 418.791396 491.718526 \n",
       "L 419.6109 498.993223 \n",
       "L 420.430403 499.113705 \n",
       "L 421.249907 503.020995 \n",
       "L 422.069411 512.450823 \n",
       "L 422.888914 503.765736 \n",
       "L 423.708418 511.711961 \n",
       "L 424.527921 508.735172 \n",
       "L 425.347425 513.437595 \n",
       "L 426.166929 509.454347 \n",
       "L 426.986432 511.264897 \n",
       "L 427.805936 517.832585 \n",
       "L 428.625439 512.567201 \n",
       "L 429.444943 514.180535 \n",
       "L 430.264447 517.418313 \n",
       "L 431.08395 508.687056 \n",
       "L 431.903454 511.104222 \n",
       "L 432.722957 514.949056 \n",
       "L 433.542461 517.315904 \n",
       "L 434.361965 519.068978 \n",
       "L 435.181468 514.55431 \n",
       "L 436.000972 520.081455 \n",
       "L 436.820475 523.120756 \n",
       "L 437.639979 514.215853 \n",
       "L 438.459483 516.529352 \n",
       "L 439.278986 524.960387 \n",
       "L 440.09849 517.516576 \n",
       "L 440.917993 517.040151 \n",
       "L 441.737497 522.602006 \n",
       "L 442.557001 518.345304 \n",
       "L 443.376504 521.149742 \n",
       "L 444.196008 520.830376 \n",
       "L 445.015511 512.90601 \n",
       "L 445.835015 518.145629 \n",
       "L 446.654519 512.914122 \n",
       "L 447.474022 519.874128 \n",
       "L 449.113029 517.346604 \n",
       "L 449.932533 523.09817 \n",
       "L 450.752037 515.295219 \n",
       "L 451.57154 518.164166 \n",
       "L 452.391044 517.272382 \n",
       "L 453.210547 517.194625 \n",
       "L 454.030051 523.086731 \n",
       "L 454.849555 519.470077 \n",
       "L 455.669058 513.531448 \n",
       "L 456.488562 524.01512 \n",
       "L 457.308065 517.637195 \n",
       "L 458.127569 513.791306 \n",
       "L 458.947073 515.580888 \n",
       "L 459.766576 520.606595 \n",
       "L 460.58608 520.297109 \n",
       "L 461.405583 520.47927 \n",
       "L 462.225087 515.983663 \n",
       "L 463.044591 516.267252 \n",
       "L 463.864094 517.571993 \n",
       "L 464.683598 513.128294 \n",
       "L 465.503101 523.893442 \n",
       "L 466.322605 518.63725 \n",
       "L 467.142109 516.138454 \n",
       "L 467.961612 524.626242 \n",
       "L 468.781116 518.663289 \n",
       "L 469.600619 522.178654 \n",
       "L 470.420123 519.141121 \n",
       "L 471.239627 524.085059 \n",
       "L 472.05913 520.166401 \n",
       "L 472.878634 522.298607 \n",
       "L 473.698137 522.027582 \n",
       "L 474.517641 525.726673 \n",
       "L 475.337145 521.41985 \n",
       "L 476.156648 522.515904 \n",
       "L 476.976152 517.511326 \n",
       "L 477.795655 516.420672 \n",
       "L 478.615159 526.521475 \n",
       "L 479.434663 521.333403 \n",
       "L 480.254166 521.284615 \n",
       "L 481.07367 520.563349 \n",
       "L 481.893173 527.35464 \n",
       "L 482.712677 528.743304 \n",
       "L 483.532181 521.87505 \n",
       "L 484.351684 523.649093 \n",
       "L 485.171188 528.577646 \n",
       "L 485.990691 530.910738 \n",
       "L 486.810195 517.71681 \n",
       "L 487.629699 525.1781 \n",
       "L 488.449202 520.708472 \n",
       "L 489.268706 533.803654 \n",
       "L 490.088209 523.91883 \n",
       "L 490.907713 522.356422 \n",
       "L 491.727217 523.785943 \n",
       "L 492.54672 524.863211 \n",
       "L 493.366224 520.045028 \n",
       "L 494.185727 526.227126 \n",
       "L 495.005231 514.984863 \n",
       "L 495.824734 525.247857 \n",
       "L 496.644238 525.214201 \n",
       "L 497.463742 521.073378 \n",
       "L 498.283245 521.18234 \n",
       "L 499.102749 519.272157 \n",
       "L 499.922252 527.423216 \n",
       "L 500.741756 522.03416 \n",
       "L 501.56126 512.459867 \n",
       "L 502.380763 522.494037 \n",
       "L 503.200267 524.251008 \n",
       "L 504.01977 520.454444 \n",
       "L 504.839274 521.359217 \n",
       "L 505.658778 521.336574 \n",
       "L 506.478281 525.366659 \n",
       "L 507.297785 519.03752 \n",
       "L 508.117288 523.511714 \n",
       "L 508.936792 524.77392 \n",
       "L 509.756296 523.236788 \n",
       "L 510.575799 519.784136 \n",
       "L 511.395303 520.853053 \n",
       "L 512.214806 525.620224 \n",
       "L 513.03431 522.629715 \n",
       "L 513.853814 527.036102 \n",
       "L 516.312324 524.339822 \n",
       "L 517.951332 529.97117 \n",
       "L 518.770835 515.776018 \n",
       "L 519.590339 521.859163 \n",
       "L 520.409842 532.788558 \n",
       "L 521.229346 523.683687 \n",
       "L 522.04885 529.348236 \n",
       "L 522.868353 529.034706 \n",
       "L 523.687857 517.889391 \n",
       "L 524.50736 520.250359 \n",
       "L 525.326864 530.8806 \n",
       "L 526.146368 526.182172 \n",
       "L 526.965871 529.971925 \n",
       "L 527.785375 527.773567 \n",
       "L 528.604878 532.970037 \n",
       "L 529.424382 528.400999 \n",
       "L 530.243886 527.781039 \n",
       "L 531.063389 527.872416 \n",
       "L 531.882893 536.333692 \n",
       "L 532.702396 528.030092 \n",
       "L 533.5219 525.268935 \n",
       "L 534.341404 528.413181 \n",
       "L 535.160907 528.31474 \n",
       "L 535.980411 532.207866 \n",
       "L 536.799914 527.900304 \n",
       "L 537.619418 527.980933 \n",
       "L 538.438922 530.085585 \n",
       "L 539.258425 522.684446 \n",
       "L 540.077929 518.517838 \n",
       "L 540.897432 518.965165 \n",
       "L 541.716936 530.189504 \n",
       "L 542.53644 528.608049 \n",
       "L 543.355943 521.803471 \n",
       "L 544.175447 521.586705 \n",
       "L 544.99495 523.182491 \n",
       "L 545.814454 530.274906 \n",
       "L 547.453461 527.814567 \n",
       "L 548.272965 523.307813 \n",
       "L 549.092468 529.819355 \n",
       "L 549.911972 524.802965 \n",
       "L 550.731476 528.13462 \n",
       "L 551.550979 526.95155 \n",
       "L 552.370483 522.802055 \n",
       "L 553.189986 521.270884 \n",
       "L 554.00949 530.531852 \n",
       "L 554.828994 526.137266 \n",
       "L 555.648497 517.958831 \n",
       "L 556.468001 520.790087 \n",
       "L 557.287504 525.653562 \n",
       "L 558.107008 536.4 \n",
       "L 558.926512 521.529525 \n",
       "L 559.746015 524.486476 \n",
       "L 560.565519 530.269896 \n",
       "L 561.385022 515.914865 \n",
       "L 562.204526 527.668727 \n",
       "L 563.02403 530.201685 \n",
       "L 563.843533 528.91027 \n",
       "L 564.663037 530.622177 \n",
       "L 565.48254 528.612675 \n",
       "L 566.302044 532.827178 \n",
       "L 567.121548 530.815783 \n",
       "L 567.941051 526.698355 \n",
       "L 567.941051 526.698355 \n",
       "\" clip-path=\"url(#p49ad460a07)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 2; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 35.304688 561.6 \n",
       "L 35.304688 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 593.304688 561.6 \n",
       "L 593.304688 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 35.304688 561.6 \n",
       "L 593.304688 561.6 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 35.304688 7.2 \n",
       "L 593.304688 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 516.2 44.55625 \n",
       "L 586.304688 44.55625 \n",
       "Q 588.304688 44.55625 588.304688 42.55625 \n",
       "L 588.304688 14.2 \n",
       "Q 588.304688 12.2 586.304688 12.2 \n",
       "L 516.2 12.2 \n",
       "Q 514.2 12.2 514.2 14.2 \n",
       "L 514.2 42.55625 \n",
       "Q 514.2 44.55625 516.2 44.55625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_16\">\n",
       "     <path d=\"M 518.2 20.298438 \n",
       "L 528.2 20.298438 \n",
       "L 538.2 20.298438 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 2; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- MC loss -->\n",
       "     <g transform=\"translate(546.2 23.798438) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-4d\" d=\"M 628 4666 \n",
       "L 1569 4666 \n",
       "L 2759 1491 \n",
       "L 3956 4666 \n",
       "L 4897 4666 \n",
       "L 4897 0 \n",
       "L 4281 0 \n",
       "L 4281 4097 \n",
       "L 3078 897 \n",
       "L 2444 897 \n",
       "L 1241 4097 \n",
       "L 1241 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-43\" d=\"M 4122 4306 \n",
       "L 4122 3641 \n",
       "Q 3803 3938 3442 4084 \n",
       "Q 3081 4231 2675 4231 \n",
       "Q 1875 4231 1450 3742 \n",
       "Q 1025 3253 1025 2328 \n",
       "Q 1025 1406 1450 917 \n",
       "Q 1875 428 2675 428 \n",
       "Q 3081 428 3442 575 \n",
       "Q 3803 722 4122 1019 \n",
       "L 4122 359 \n",
       "Q 3791 134 3420 21 \n",
       "Q 3050 -91 2638 -91 \n",
       "Q 1578 -91 968 557 \n",
       "Q 359 1206 359 2328 \n",
       "Q 359 3453 968 4101 \n",
       "Q 1578 4750 2638 4750 \n",
       "Q 3056 4750 3426 4639 \n",
       "Q 3797 4528 4122 4306 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-4d\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-43\" x=\"86.279297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"156.103516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"187.890625\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"215.673828\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"276.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"328.955078\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_17\">\n",
       "     <path d=\"M 518.2 34.976563 \n",
       "L 528.2 34.976563 \n",
       "L 538.2 34.976563 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 2; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- val -->\n",
       "     <g transform=\"translate(546.2 38.476563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p49ad460a07\">\n",
       "   <rect x=\"35.304688\" y=\"7.2\" width=\"558\" height=\"554.4\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Loss: -71.52269058227539\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TRAIN MC MODEL\n",
    "'''\n",
    "MC_loss_hist, MC_val_hist, full_loss_hist, MC_full_val_hist = train(training_data_MC, MC_model, val = True, val_data = val_data_MC, num_epochs = 11, compact_num = 20)\n",
    "plot_loss(MC_loss_hist, label = \"MC loss\",plot_val = True, val_loss_hist = MC_val_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aba59b-22a9-41f3-8379-25ca3bd5b0c8",
   "metadata": {},
   "source": [
    "You can save the trained model with the line below, using the *save()* method of the *nf.NormalizingFlow* class, where the only argument is the save file location\n",
    "\n",
    "(note: save models with \\*.pth file type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de5608a8-45c5-4351-b09a-7d221a40bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC_model.save(\"models/MC_test.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0882df53-e37f-4ed2-b745-1d563be4254e",
   "metadata": {},
   "source": [
    "To load a previously saved model, uncomment the cell below\n",
    "\n",
    "This cell constructs the flows and model objects, but loads the model with the saved info instead of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12193fc5-8010-4ca0-820c-0b9a403bbe71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLOAD SAVED MC MODEL\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "LOAD SAVED MC MODEL\n",
    "'''\n",
    "# masked_affine_flows_train_MC = get_masked_affine(52)\n",
    "# distribution_MC = nf.distributions.DiagGaussian(training_data_MC.latent_size, trainable = False)\n",
    "# masked_affine_model_MC = nf.NormalizingFlow(q0=distribution_MC, flows=masked_affine_flows_train_MC)\n",
    "# masked_affine_model_MC.load(\"models/MC_test.pth\")\n",
    "# MC_model = masked_affine_model_MC.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0042c21-d8b1-4dd1-bb0f-687376aa9084",
   "metadata": {},
   "source": [
    "Here we can test the MC model on our testing segment of the dataset\n",
    "\n",
    "The *test()* function from *NF_utils.py* prints the average loss across all batches tested (if this is much higher than the lowest training loss, something may have gone wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f9a2371-7b03-4ef3-8423-213f0e003bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [00:03<00:00, 44.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC average loss: -67.61296844482422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TESTING MC MODEL\n",
    "'''\n",
    "test(testing_data_MC, MC_model, data_type = \"MC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dbe816-28ae-434c-ba13-49da1650ccd4",
   "metadata": {},
   "source": [
    "The following cells do the same things as the previous 4 cells, now for DATA instead of MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe296596-06aa-4c75-b83b-5b4de484d5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1992/1992 [04:40<00:00,  7.10it/s]\n",
      "100%|██████████| 1992/1992 [04:15<00:00,  7.80it/s]\n",
      "100%|██████████| 1992/1992 [04:01<00:00,  8.26it/s]\n",
      " 27%|██▋       | 532/1992 [01:03<02:55,  8.34it/s]"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TRAINING DATA MODEL\n",
    "'''\n",
    "DATA_loss_hist, DATA_val_hist, DATA_full_loss_hist, DATA_full_val_hist = train(training_data_DATA, DATA_model, val = True, val_data = val_data_DATA, num_epochs = 5, compact_num = 20)\n",
    "plot_loss(DATA_loss_hist, label = \"DATA loss\", plot_val = True, val_loss_hist = DATA_val_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba1b7b-5c00-42b2-a2cf-68ac3ae324bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SAVE DATA MODEL\n",
    "'''\n",
    "# DATA_model.save(\"models/DATA_test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4651703b-fad2-425e-b5f3-6f2452c82565",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LOAD SAVED DATA MODEL\n",
    "'''\n",
    "# masked_affine_flows_train_DATA = get_masked_affine(52)\n",
    "# distribution_DATA = nf.distributions.DiagGaussian(training_data_DATA.latent_size, trainable = False)\n",
    "# masked_affine_model_DATA = nf.NormalizingFlow(q0=distribution_DATA, flows=masked_affine_flows_train_DATA)\n",
    "# masked_affine_model_DATA.load(\"models/DATA_test.pth\")\n",
    "# DATA_model = masked_affine_model_DATA.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ec245-fd43-4feb-88b1-ba11f9226494",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TESTING DATA MODEL\n",
    "'''\n",
    "test(testing_data_DATA, DATA_model, data_type = \"DATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484711c-63c3-4aae-8da5-38b17e59d998",
   "metadata": {},
   "source": [
    "The next cell uses the trained NF models to transform data from the Latent domain to a normalized domain, and then to the MC latent domain (fullpass)\n",
    "\n",
    "This uses the *transform()* function from *NF_Utils.py* to iteratively pass through each batch through the trained models, and stores the outputs in new *Latent_data* objects\n",
    "\n",
    "Note: the MC data doesn't necessarily need to be transformed, only if using for something specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9718d892-c5b1-4f3e-8240-9e93e5f1865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TRANSFORMING\n",
    "'''\n",
    "\n",
    "transformed_latent_DATA = transform(testing_data_DATA, DATA_model)\n",
    "transformed_latent_MC = transform(testing_data_MC, MC_model)\n",
    "\n",
    "'''training and validation datasets can be transformed as well if needed'''\n",
    "\n",
    "transformed_latent_train_MC = transform(training_data_MC, MC_model)\n",
    "transformed_latent_train_DATA = transform(training_data_DATA, DATA_model)\n",
    "transformed_latent_val_MC = transform(val_data_MC, MC_model)\n",
    "transformed_latent_val_DATA = transform(val_data_DATA, DATA_model)\n",
    "\n",
    "'''transform() returns a torch.tensor object, so we use these to create new Latent_data objects that know the labels'''\n",
    "transformed_latent_DATA_obj = Latent_data(transformed_latent_DATA,testing_data_DATA.labels)\n",
    "transformed_latent_DATA_obj.set_batch_size(num_samples)\n",
    "transformed_latent_MC_obj = Latent_data(transformed_latent_MC,testing_data_MC.labels)\n",
    "transformed_latent_MC_obj.set_batch_size(num_samples)\n",
    "\n",
    "transformed_latent_train_DATA_obj = Latent_data(transformed_latent_train_DATA,training_data_DATA.labels)\n",
    "transformed_latent_train_DATA_obj.set_batch_size(num_samples)\n",
    "transformed_latent_train_MC_obj = Latent_data(transformed_latent_train_MC,training_data_MC.labels)\n",
    "transformed_latent_train_MC_obj.set_batch_size(num_samples)\n",
    "\n",
    "transformed_latent_val_DATA_obj = Latent_data(transformed_latent_val_DATA,val_data_DATA.labels)\n",
    "transformed_latent_val_MC_obj = Latent_data(transformed_latent_val_MC,val_data_MC.labels)\n",
    "\n",
    "'''pass the normalized data through \"forward\" direction of MC to complete full transformation'''\n",
    "full_pass_DATA = transform(transformed_latent_DATA_obj, MC_model, reverse = False)\n",
    "full_pass_DATA_obj = Latent_data(full_pass_DATA, testing_data_DATA.labels)\n",
    "full_pass_DATA_obj.set_batch_size(num_samples)\n",
    "\n",
    "full_pass_train_DATA = transform(transformed_latent_train_DATA_obj, MC_model, reverse = False)\n",
    "full_pass_train_DATA_obj = Latent_data(full_pass_train_DATA, training_data_DATA.labels)\n",
    "full_pass_train_DATA_obj.set_batch_size(num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736e60a8-a31c-4ab8-ba68-cc8e52e9ec85",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Now we can predict if Lambda events are signal or background with a classification network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25e85f-45d3-4b3c-a9d7-50d00bc31bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CLASSIFIER CONSTRUCTION\n",
    "'''\n",
    "\n",
    "classifier = NFClassifier(num_layers = 10).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "num_epochs_classifier = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d960c-d345-43a5-9422-a0d0633d63f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PLOTTING CLASSIFIER LOSS\n",
    "'''\n",
    "\n",
    "loss_hist, val_loss_hist = train_classifier(training_data_MC, classifier, criterion, optimizer, val = True, val_data = val_data_MC, num_epochs = num_epochs_classifier)\n",
    "\n",
    "# Can save the loss-plot with the save and save_loc optional arguments\n",
    "plot_loss(loss_hist, plot_val =True, val_loss_hist = val_loss_hist,save = True, save_loc = \"plots/test/class_loss_plot.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17ac323-d9a7-46fe-8949-39199bf47482",
   "metadata": {},
   "source": [
    "Below we can test the classifier performance on the testing segment of our MC data to determine our accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa2524-407d-4684-b110-f5c71f2a57eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TESTING CLASSIFIER PERFORMANCE ON MC\n",
    "'''\n",
    "test_classifier_MC(testing_data_MC,classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6a5c41-0782-45d1-b38c-35ff41c0c3ac",
   "metadata": {},
   "source": [
    "Now we classify our latent data to extract signal and bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934174f-5638-4015-8e37-38e8a2df2847",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_DATA_test = get_classification_probs(full_pass_DATA_obj, classifier)\n",
    "probs_DATA_fp = get_classification_probs(full_pass_train_DATA_obj, classifier)\n",
    "probs_DATA = get_classification_probs(training_data_DATA, classifier)\n",
    "probs_MC = get_classification_probs(training_data_MC, classifier)\n",
    "probs_Y = get_classification_probs(testing_data_MC, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4200e29f-843d-44b2-8047-f8c1f52fd289",
   "metadata": {},
   "source": [
    "Below we calculate the ROC curve and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1b255-2b63-479c-96cd-f52899835113",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "# Get ROC curve\n",
    "test_Y = testing_data_MC.labels\n",
    "# test_Y_normalized = transformed_latent_MC_obj.labels\n",
    "# test_Y_normalized = transformed_latent_MC_obj.labels\n",
    "# pfn_fp, pfn_tp, threshs = roc_curve(test_Y.detach().to(\"cpu\").numpy(), probs_Y[:,1].detach().numpy())\n",
    "pfn_fp, pfn_tp, threshs = roc_curve(test_Y.detach().to(\"cpu\").numpy(), probs_Y[:,1].detach().numpy())\n",
    "\n",
    "# Get area under the ROC curve\n",
    "\n",
    "# auc = roc_auc_score(np.squeeze(test_Y.detach().to(\"cpu\").numpy()), probs_Y[:,1].detach().numpy())\n",
    "auc = roc_auc_score(np.squeeze(test_Y.detach().to(\"cpu\").numpy()), probs_Y[:,1].detach().numpy())\n",
    "if verbose: print(f'AUC = {auc:.4f}')\n",
    "# if verbose: print(f'test_acc = {test_acc:.4f}')#DEBUGGING ADDED\n",
    "\n",
    "# Create matplotlib plots for ROC curve and testing decisions\n",
    "f = plt.figure()\n",
    "\n",
    "# Get some nicer plot settings \n",
    "# plt.rcParams['figure.figsize'] = (4,4)#DEBUGGING\n",
    "# f.rcParams['font.family'] = 'serif'\n",
    "# f.rcParams['figure.autolayout'] = True\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(pfn_tp, 1-pfn_fp, '-', color='black')\n",
    "\n",
    "# axes labels\n",
    "plt.xlabel('Lambda Event Efficiency')\n",
    "plt.ylabel('Background Rejection')\n",
    "\n",
    "# axes limits\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# make legend and show plot\n",
    "# plt.legend([model.name+f\": AUC={auc:.4f} acc={test_acc:.4f}\"],loc='lower left', frameon=False)\n",
    "f.show()\n",
    "# f.savefig(os.path.join(log_dir,model.name+\"_ROC_\"+datetime.datetime.now().strftime(\"%F\")+\".png\"))\n",
    "# f.savefig(\"plots/paper_plots/roc_curves/MC_Aug_14.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13978c70-98c9-4301-bbbd-7ef095c58dd2",
   "metadata": {},
   "source": [
    "Here we can plot the classifier output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c73abc-74c4-4412-8968-46b94631cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.hist(probs_DATA_test[:,1].detach().to(\"cpu\"),color = \"blue\", alpha = 0.5, bins = 100,density = True, label = \"full pass\");\n",
    "ax.hist(probs_MC[:,1].detach().to(\"cpu\"), color = \"orange\", alpha = 0.5, bins = 100,density = True, label = \"latent MC\");\n",
    "ax.hist(probs_DATA[:,1].detach().to(\"cpu\"), color = \"green\", alpha = 0.5, bins = 100,density = True, label = \"latent DATA\");\n",
    "ax.legend()\n",
    "fig.show()\n",
    "# fig.savefig(\"plots/paper_plots/output_distributions/August_14_three.jpeg\",dpi = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9fc028-d3ab-4d6d-b0c7-48255da6e19a",
   "metadata": {},
   "source": [
    "Below we can place a cut on the classifier output to view the signal and bg fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed44db-7118-44d6-b346-4d5cae90aece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cut_hist = torch.empty(15,3)\n",
    "roc_cut = 0.1\n",
    "probs_data = probs_MC\n",
    "# probs_data = probs_DATA_fp\n",
    "# probs_data = probs_DATA\n",
    "#     if(i > 3):\n",
    "#         continue\n",
    "if(roc_cut == 0):\n",
    "    argmax_Y = torch.max(probs_data, 1)[1]\n",
    "else:\n",
    "    argmax_Y = torch.tensor([1 if el>roc_cut else 0 for el in probs_data[:,1]],dtype=torch.long)\n",
    "\n",
    "masked_mass = argmax_Y * training_data_MC.mass\n",
    "# masked_mass = argmax_Y * training_data_DATA.mass\n",
    "#     signal_mass = argmax_Y * testing_data_DATA.mass\n",
    "signal_mass = np.array([])\n",
    "for j in range(masked_mass.size()[0]):\n",
    "    if(masked_mass[j] != 0):\n",
    "        signal_mass = np.append(signal_mass, masked_mass[j])\n",
    "# Define fit function\n",
    "\n",
    "\n",
    "low_high = (1.08,1.24)\n",
    "bins = 100\n",
    "\n",
    "# hdata = np.histogram(signal_mass, range=low_high, bins=bins, density=False);\n",
    "hdata = plt.hist(signal_mass, color='tab:orange', alpha=0.5, range=low_high, bins=bins, histtype='stepfilled', density=False, label='signal');\n",
    "\n",
    "N, beta, m, loc, scale, A, B, C = 10, 1, 1.112, 1.115, 0.008, np.average(hdata[0][-10:-1]), 37, 1.24\n",
    "d_N, d_beta, d_m, d_loc, d_scale, d_A, d_B, d_C = N/0.01, beta/0.1, m/0.1, loc/0.1, scale/0.01, A/10, B/0.1, C/1\n",
    "parsMin = [N-d_N, beta-d_beta, m-d_m, loc-d_loc, scale-d_scale, B-d_B]\n",
    "parsMax = [N+d_N, beta+d_beta, m+d_m, loc+d_loc, scale+d_scale, B+d_B]\n",
    "\n",
    "def func(x, N, beta, m, loc, scale, B, A=A, C=C):\n",
    "    return N*crystalball.pdf(-x, beta, m, -loc, scale) + A*(1 - B*(x - C)**2)\n",
    "\n",
    "def sig(x, N, beta, m, loc, scale):\n",
    "    return N*crystalball.pdf(-x, beta, m, -loc, scale)\n",
    "\n",
    "def bg(x, B, A=A, C=C):\n",
    "    return A*(1 - B*(x - C)**2)\n",
    "\n",
    "optParams, pcov = opt.curve_fit(func, hdata[1][:-1], hdata[0], method='trf', bounds=(parsMin,parsMax))\n",
    "\n",
    "x = np.linspace(low_high[0],low_high[1],bins)\n",
    "y = hdata[0]\n",
    "\n",
    "plt.plot(x, func(x, *optParams), color='r')\n",
    "plt.plot(x, sig(x, *optParams[0:5]), color='tab:purple')\n",
    "plt.plot(x, bg(x, *optParams[5:]), color='b')\n",
    "\n",
    "# bghist = np.histogram(x, weights=y-bg(x, *optParams[5:]), bins=bins, range=low_high);\n",
    "bghist = plt.hist(x, weights=y-bg(x, *optParams[5:]), bins=bins, range=low_high, histtype='step', alpha=0.5, color='b');\n",
    "# plt.savefig(\"plots/bghist_0_07_diff.jpeg\")\n",
    "\n",
    "r = np.divide(y - func(x, *optParams),np.sqrt([el if el>0 else 1 for el in func(x, *optParams)]))\n",
    "chi2 = np.sum(np.square(r))\n",
    "chi2ndf = chi2/len(optParams)\n",
    "\n",
    "# Get S and N before and after? #DEBUGGING: ADDED\n",
    "import scipy.integrate as integrate\n",
    "mu      = optParams[3]\n",
    "sigma   = optParams[4]\n",
    "mmin    = mu - 2*sigma\n",
    "mmax    = mu + 2*sigma\n",
    "\n",
    "\n",
    "binwidth = (low_high[1]-low_high[0])/bins#KEEP!!!\n",
    "\n",
    "bin1 = int((mmin-low_high[0])/binwidth)\n",
    "bin2 = int((mmax-low_high[0])/binwidth)\n",
    "\n",
    "integral_bghist = sum(bghist[0][bin1:bin2])\n",
    "\n",
    "integral_tothist = sum(hdata[0][bin1:bin2])\n",
    "try:\n",
    "    fom = integral_bghist/np.sqrt(integral_tothist)\n",
    "    purity =(integral_bghist)/integral_tothist\n",
    "except Exception as inst:\n",
    "     print(f\"Caught {inst} | skipping cut #{i} = {roc_cut}\")\n",
    "print(f\"roc_cut = {roc_cut} | FOM: {fom} | purity: {purity}\")\n",
    "# plt.savefig(\"plots/paper_plots/mass_fit/MC_August_14_avg.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb2520-9c58-48be-8462-4f97226d1977",
   "metadata": {},
   "source": [
    "Now we can calculate the purity and figure of merit as functions of the cut output\n",
    "\n",
    "To do this we cut the classifier at each cut value and fit the resulting mass spectrums with signal and bg functions, then integrate over the bins contained in these functions to estimate the number of signal and bg events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c72f7-be94-4212-8d07-d431e7124e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 20\n",
    "fp_cut_hist = torch.zeros(num_points,3)\n",
    "\n",
    "for i in range(num_points):\n",
    "    roc_cut = i/num_points\n",
    "#     probs_data = probs_MC\n",
    "#     probs_data = probs_DATA_fp\n",
    "    probs_data = probs_DATA_test\n",
    "#     probs_data = probs_DATA\n",
    "    #     if(i > 3):\n",
    "    #         continue\n",
    "#     if(roc_cut == 0):\n",
    "#         argmax_Y = torch.max(probs_data, 1)[1]\n",
    "#     else:\n",
    "#         argmax_Y = torch.tensor([1 if el>roc_cut else 0 for el in probs_data[:,1]],dtype=torch.long)\n",
    "    argmax_Y = torch.tensor([1 if el>roc_cut else 0 for el in probs_data[:,1]],dtype=torch.long)\n",
    "#     masked_mass = argmax_Y * training_data_MC.mass\n",
    "#     masked_mass = argmax_Y * training_data_DATA.mass\n",
    "    signal_mass = argmax_Y * testing_data_DATA.mass\n",
    "    signal_mass = np.array([])\n",
    "    for j in range(masked_mass.size()[0]):\n",
    "        if(masked_mass[j] != 0):\n",
    "            signal_mass = np.append(signal_mass, masked_mass[j])\n",
    "    # Define fit function\n",
    "\n",
    "\n",
    "    low_high = (1.08,1.24)\n",
    "    bins = 100\n",
    "\n",
    "    hdata = np.histogram(signal_mass, range=low_high, bins=bins, density=False);\n",
    "#     hdata = plt.hist(signal_mass, color='tab:orange', alpha=0.5, range=low_high, bins=bins, histtype='stepfilled', density=False, label='signal');\n",
    "\n",
    "    N, beta, m, loc, scale, A, B, C = 10, 1, 1.112, 1.115, 0.008, np.average(hdata[0][-10:-1]), 37, 1.24\n",
    "    d_N, d_beta, d_m, d_loc, d_scale, d_A, d_B, d_C = N/0.01, beta/0.1, m/0.1, loc/0.1, scale/0.01, A/10, B/0.1, C/1\n",
    "    parsMin = [N-d_N, beta-d_beta, m-d_m, loc-d_loc, scale-d_scale, B-d_B]\n",
    "    parsMax = [N+d_N, beta+d_beta, m+d_m, loc+d_loc, scale+d_scale, B+d_B]\n",
    "\n",
    "    def func(x, N, beta, m, loc, scale, B, A=A, C=C):\n",
    "        return N*crystalball.pdf(-x, beta, m, -loc, scale) + A*(1 - B*(x - C)**2)\n",
    "\n",
    "    def sig(x, N, beta, m, loc, scale):\n",
    "        return N*crystalball.pdf(-x, beta, m, -loc, scale)\n",
    "\n",
    "    def bg(x, B, A=A, C=C):\n",
    "        return A*(1 - B*(x - C)**2)\n",
    "    try:\n",
    "        optParams, pcov = opt.curve_fit(func, hdata[1][:-1], hdata[0], method='trf', bounds=(parsMin,parsMax))\n",
    "    except Exception as E:\n",
    "        print(f\"Exception caught: {E}; skipping cut: {roc_cut}\")\n",
    "        continue\n",
    "\n",
    "    x = np.linspace(low_high[0],low_high[1],bins)\n",
    "    y = hdata[0]\n",
    "\n",
    "#     plt.plot(x, func(x, *optParams), color='r')\n",
    "#     plt.plot(x, sig(x, *optParams[0:5]), color='tab:purple')\n",
    "#     plt.plot(x, bg(x, *optParams[5:]), color='b')\n",
    "\n",
    "    bghist = np.histogram(x, weights=y-bg(x, *optParams[5:]), bins=bins, range=low_high);\n",
    "#     bghist = plt.hist(x, weights=y-bg(x, *optParams[5:]), bins=bins, range=low_high, histtype='step', alpha=0.5, color='b');\n",
    "    # plt.savefig(\"plots/bghist_0_07_diff.jpeg\")\n",
    "\n",
    "    r = np.divide(y - func(x, *optParams),np.sqrt([el if el>0 else 1 for el in func(x, *optParams)]))\n",
    "    chi2 = np.sum(np.square(r))\n",
    "    chi2ndf = chi2/len(optParams)\n",
    "\n",
    "    # Get S and N before and after? #DEBUGGING: ADDED\n",
    "    import scipy.integrate as integrate\n",
    "    mu      = optParams[3]\n",
    "    sigma   = optParams[4]\n",
    "    mmin    = mu - 2*sigma\n",
    "    mmax    = mu + 2*sigma\n",
    "\n",
    "\n",
    "    binwidth = (low_high[1]-low_high[0])/bins#KEEP!!!\n",
    "\n",
    "    bin1 = int((mmin-low_high[0])/binwidth)\n",
    "    bin2 = int((mmax-low_high[0])/binwidth)\n",
    "\n",
    "    integral_bghist = sum(bghist[0][bin1:bin2])\n",
    "\n",
    "    integral_tothist = sum(hdata[0][bin1:bin2])\n",
    "    try:\n",
    "        fom = integral_bghist/np.sqrt(integral_tothist)\n",
    "        purity =(integral_bghist)/integral_tothist\n",
    "        fp_cut_hist[i] = torch.tensor([roc_cut, fom, purity])\n",
    "    except Exception as inst:\n",
    "        print(f\"Caught {inst} | skipping cut #{i} = {roc_cut}\")\n",
    "        \n",
    "#     print(f\"roc_cut = {roc_cut} | FOM: {fom} | purity: {purity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebb2ecf-916d-4788-bee7-84de55a5b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 20\n",
    "data_cut_hist = torch.zeros(num_points,3)\n",
    "\n",
    "for i in range(num_points):\n",
    "    roc_cut = i/num_points\n",
    "#     probs_data = probs_MC\n",
    "#     probs_data = probs_DATA_fp\n",
    "    probs_data = probs_DATA\n",
    "    #     if(i > 3):\n",
    "    #         continue\n",
    "#     if(roc_cut == 0):\n",
    "#         argmax_Y = torch.max(probs_data, 1)[1]\n",
    "#     else:\n",
    "#         argmax_Y = torch.tensor([1 if el>roc_cut else 0 for el in probs_data[:,1]],dtype=torch.long)\n",
    "    argmax_Y = torch.tensor([1 if el>roc_cut else 0 for el in probs_data[:,1]],dtype=torch.long)\n",
    "#     masked_mass = argmax_Y * training_data_MC.mass\n",
    "    masked_mass = argmax_Y * training_data_DATA.mass\n",
    "    #     signal_mass = argmax_Y * testing_data_DATA.mass\n",
    "    signal_mass = np.array([])\n",
    "    for j in range(masked_mass.size()[0]):\n",
    "        if(masked_mass[j] != 0):\n",
    "            signal_mass = np.append(signal_mass, masked_mass[j])\n",
    "    # Define fit function\n",
    "\n",
    "\n",
    "    low_high = (1.08,1.24)\n",
    "    bins = 100\n",
    "\n",
    "    hdata = np.histogram(signal_mass, range=low_high, bins=bins, density=False);\n",
    "#     hdata = plt.hist(signal_mass, color='tab:orange', alpha=0.5, range=low_high, bins=bins, histtype='stepfilled', density=False, label='signal');\n",
    "\n",
    "    N, beta, m, loc, scale, A, B, C = 10, 1, 1.112, 1.115, 0.008, np.average(hdata[0][-10:-1]), 37, 1.24\n",
    "    d_N, d_beta, d_m, d_loc, d_scale, d_A, d_B, d_C = N/0.01, beta/0.1, m/0.1, loc/0.1, scale/0.01, A/10, B/0.1, C/1\n",
    "    parsMin = [N-d_N, beta-d_beta, m-d_m, loc-d_loc, scale-d_scale, B-d_B]\n",
    "    parsMax = [N+d_N, beta+d_beta, m+d_m, loc+d_loc, scale+d_scale, B+d_B]\n",
    "\n",
    "    def func(x, N, beta, m, loc, scale, B, A=A, C=C):\n",
    "        return N*crystalball.pdf(-x, beta, m, -loc, scale) + A*(1 - B*(x - C)**2)\n",
    "\n",
    "    def sig(x, N, beta, m, loc, scale):\n",
    "        return N*crystalball.pdf(-x, beta, m, -loc, scale)\n",
    "\n",
    "    def bg(x, B, A=A, C=C):\n",
    "        return A*(1 - B*(x - C)**2)\n",
    "    try:\n",
    "        optParams, pcov = opt.curve_fit(func, hdata[1][:-1], hdata[0], method='trf', bounds=(parsMin,parsMax))\n",
    "    except Exception as E:\n",
    "        print(f\"Exception caught: {E}; skipping cut: {roc_cut}\")\n",
    "        continue\n",
    "\n",
    "    x = np.linspace(low_high[0],low_high[1],bins)\n",
    "    y = hdata[0]\n",
    "\n",
    "#     plt.plot(x, func(x, *optParams), color='r')\n",
    "#     plt.plot(x, sig(x, *optParams[0:5]), color='tab:purple')\n",
    "#     plt.plot(x, bg(x, *optParams[5:]), color='b')\n",
    "\n",
    "    bghist = np.histogram(x, weights=y-bg(x, *optParams[5:]), bins=bins, range=low_high);\n",
    "#     bghist = plt.hist(x, weights=y-bg(x, *optParams[5:]), bins=bins, range=low_high, histtype='step', alpha=0.5, color='b');\n",
    "    # plt.savefig(\"plots/bghist_0_07_diff.jpeg\")\n",
    "\n",
    "    r = np.divide(y - func(x, *optParams),np.sqrt([el if el>0 else 1 for el in func(x, *optParams)]))\n",
    "    chi2 = np.sum(np.square(r))\n",
    "    chi2ndf = chi2/len(optParams)\n",
    "\n",
    "    # Get S and N before and after? #DEBUGGING: ADDED\n",
    "    import scipy.integrate as integrate\n",
    "    mu      = optParams[3]\n",
    "    sigma   = optParams[4]\n",
    "    mmin    = mu - 2*sigma\n",
    "    mmax    = mu + 2*sigma\n",
    "\n",
    "\n",
    "    binwidth = (low_high[1]-low_high[0])/bins#KEEP!!!\n",
    "\n",
    "    bin1 = int((mmin-low_high[0])/binwidth)\n",
    "    bin2 = int((mmax-low_high[0])/binwidth)\n",
    "\n",
    "    integral_bghist = sum(bghist[0][bin1:bin2])\n",
    "\n",
    "    integral_tothist = sum(hdata[0][bin1:bin2])\n",
    "    try:\n",
    "        fom = integral_bghist/np.sqrt(integral_tothist)\n",
    "        purity =(integral_bghist)/integral_tothist\n",
    "        data_cut_hist[i] = torch.tensor([roc_cut, fom, purity])\n",
    "    except Exception as inst:\n",
    "        print(f\"Caught {inst} | skipping cut #{i} = {roc_cut}\")\n",
    "        \n",
    "#     print(f\"roc_cut = {roc_cut} | FOM: {fom} | purity: {purity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe226e35-b39f-4b4f-8658-d530e3afbadf",
   "metadata": {},
   "source": [
    "Now we plot the FOM and purity as functions of the cut\n",
    "\n",
    "You can save the plot via the final line using the *savefig()* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3a733-f0c4-4269-8d92-f3ab6be352b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('cut')\n",
    "ax1.scatter(fp_cut_hist[:,0], fp_cut_hist[:,1], label = \"Transformed\", color = \"tab:red\",marker = 'o', s = 15)\n",
    "ax1.set_ylabel('FOM', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_ylim([0, 50])\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('purity', color=color)  # we already handled the x-label with ax1\n",
    "ax2.scatter(fp_cut_hist[:,0], fp_cut_hist[:,2], color = \"tab:red\",marker = 'd',s = 15)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_ylim([0, 1])\n",
    "fig.legend()\n",
    "# fig.text(0.6,0.2,\"DATA\")\n",
    "fig.text(0.6,0.15,f\"{len(training_data_DATA.mass)} events\")\n",
    "ax3=ax1.twinx()\n",
    "ax3=ax1.twiny()\n",
    "color = 'tab:orange'\n",
    "# ax3.set_xlabel('cut')\n",
    "ax3.scatter(data_cut_hist[:,0], data_cut_hist[:,1], label = \"Data\", color = \"tab:blue\",marker = 'o', s = 15)\n",
    "# ax3.set_ylabel('Transformed FOM', color=color)\n",
    "ax3.tick_params(axis='y', labelcolor=color)\n",
    "ax3.set_ylim([0, 50])\n",
    "ax4 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:purple'\n",
    "# ax4.set_ylabel('Transformed purity', color=color)  # we already handled the x-label with ax1\n",
    "ax4.scatter(data_cut_hist[:,0], data_cut_hist[:,2], color = \"tab:blue\",marker = 'd',s = 15)\n",
    "ax4.tick_params(axis='y', labelcolor=color)\n",
    "ax4.set_ylim([0, 1])\n",
    "fig.legend()\n",
    "# fig.text(0.6,0.2,\"DATA\")\n",
    "fig.text(0.6,0.15,f\"{len(training_data_DATA.mass)} events\")\n",
    "# fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()\n",
    "# fig.savefig(\"plots/paper_plots/FOMpure/double/Sept_1_average_20.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d1144-7064-49a8-a69b-73a3f0377838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624dccbc-1c9b-460b-b3c3-5e0c83b47b25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
