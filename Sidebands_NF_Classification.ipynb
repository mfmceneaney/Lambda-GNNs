{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3423440-b266-4372-b031-e048056cc712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/vossenlab/rck32/miniconda3/envs/venv/lib/python3.11/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/hpc/group/vossenlab/rck32/miniconda3/envs/venv/lib/python3.11/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/hpc/group/vossenlab/rck32/miniconda3/envs/venv/lib/python3.11/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/hpc/group/vossenlab/rck32/miniconda3/envs/venv/lib/python3.11/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "2023-07-19 11:19:05.137639: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-19 11:19:14.773478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import normflows as nf\n",
    "from normflows import flows\n",
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tdata\n",
    "import torch.optim as optim\n",
    "\n",
    "import dgl #NOTE: for dgl.batch and dgl.unbatch\n",
    "from dgl import save_graphs, load_graphs\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl.data.utils import save_info, load_info, Subset\n",
    "\n",
    "import umap\n",
    "reducer = umap.UMAP();\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import scipy.optimize as opt\n",
    "from scipy.stats import crystalball\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4c7f45-97b3-4cfb-a6b5-d917770f97d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "#custom imports\n",
    "from utils import load_graph_dataset, train, evaluate, GraphDataset, get_graph_dataset_info\n",
    "from models import GIN, HeteroGIN\n",
    "from NF_utils import Latent_data, create_latent_data, get_masked_affine, transform, train,plot_loss, test,plot_9_histos, plot_UMAP_sidebyside,plot_UMAP_overlay, transform_double\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9de4853-6f7b-4236-934b-ba37c9fe20e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''                                              '''\n",
    "'''     SETTING UP LATENT SPACE REPRESENTATION   '''\n",
    "'''                                              '''\n",
    "\n",
    "#Number of graphs in each\n",
    "# DATA_max_events = 149090\n",
    "DATA_max_events = 249090\n",
    "MC_max_events = 141118\n",
    "\n",
    "# Data and MC both have the same prefix\n",
    "prefix = \"/hpc/group/vossenlab/mfm45/.dgl/\"\n",
    "\n",
    "# MC inside Lambda_train_matched_jobs_outbending_cache_bg50nA_7_28_22__pT_phi_theta_beta_chi2_pid_status__Normalized\n",
    "MCdataset = \"Lambda_train_matched_jobs_outbending_cache_bg50nA_7_28_22__pT_phi_theta_beta_chi2_pid_status__Normalized\"\n",
    "\n",
    "# Data inside data_jobs_rga_fall2018_7_28_22__pT_phi_theta_beta_chi2_pid_status__Normalized\n",
    "DATAdataset = \"data_jobs_rga_fall2018_7_28_22__pT_phi_theta_beta_chi2_pid_status__Normalized\"\n",
    "\n",
    "max_events = 1e5\n",
    "split = 0.1\n",
    "nlayers = 2\n",
    "nmlp = 3\n",
    "hdim = 64\n",
    "nclasses, nfeatures, nfeatures_edge = get_graph_dataset_info(dataset=MCdataset, prefix=prefix)\n",
    "dropout = 0.8\n",
    "learn_eps = False\n",
    "batch = 256\n",
    "indices = None\n",
    "nworkers = 0\n",
    "npooling = \"max\"\n",
    "gpooling = \"max\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "#select model\n",
    "extractor = GIN(nlayers, nmlp, nfeatures,\n",
    "            hdim, nclasses, dropout, learn_eps, npooling, gpooling).to(device)\n",
    "extractor.load_state_dict(torch.load(\"logs/model_weights\",map_location=device))\n",
    "#select training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6690176-9e03-473e-ad30-0448c90b5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Latent_data:\n",
    "    def __init__(self, in_tensor,labels, sidebands = False, num_sample_features = 71, double = False):\n",
    "        self.data = in_tensor\n",
    "        if(double):\n",
    "            self.double()\n",
    "        self.labels = labels\n",
    "        self.num_events = self.data.size()[0]\n",
    "        self.latent_size = self.data.size()[1]\n",
    "        self.num_sample_features = num_sample_features\n",
    "    def get_sidebands(self, cut = 1.14):\n",
    "        for i in range(len(self.data)):\n",
    "            if(self.mass[i] < 1.14):\n",
    "                self.data[i] = (9999 * torch.ones_like(self.data[i]))\n",
    "                self.labels[i] = (9999 * torch.ones_like(self.labels[i]))\n",
    "                self.mass[i] = (9999 * torch.ones_like(self.mass[i]))\n",
    "        self.data = self.data[self.data[:,0] != 9999]\n",
    "        self.labels = self.labels[self.labels[:,0] != 9999]\n",
    "        self.mass = self.mass[self.mass[:] != 9999]\n",
    "        self.num_events = self.data.size()[0]\n",
    "    def set_batch_size(self,batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = int(self.num_events / self.batch_size)\n",
    "    def set_mass(self, mass):\n",
    "        self.mass = mass\n",
    "    def sample(self,iteration = 0, random = False, _give_labels = False):\n",
    "        if(random):\n",
    "            return self.sample_random(give_labels = _give_labels)\n",
    "        else:\n",
    "            return self.sample_fixed(iteration,give_labels = _give_labels)\n",
    "    def sample_fixed(self,iteration,give_labels = False):\n",
    "        #0 index iterations - the \"first\" iteration is with iteration = 0\n",
    "        # Calculate the first index we want to take from training data (rest of data is directly after)\n",
    "        begin = iteration * self.batch_size\n",
    "        # initialize\n",
    "        samples = torch.zeros(self.batch_size, self.latent_size)\n",
    "        labels = torch.zeros(self.batch_size, 1)\n",
    "#         print(f\"labels max (inside sample_fixed): {labels.max()}\")\n",
    "        #loop over consecutive tensors, save to return tensor\n",
    "        if(give_labels):\n",
    "            for i in range(self.batch_size):\n",
    "                samples[i] = self.data[begin + i]\n",
    "                labels[i] = self.labels[begin+i]\n",
    "            return samples,labels\n",
    "        else:\n",
    "            for i in range(self.batch_size):\n",
    "                samples[i] = self.data[begin + i]\n",
    "            return samples\n",
    "    def sample_random(self,labels = False):\n",
    "        indices = rng.integers(low=0, high=self.num_events, size=self.batch_size)\n",
    "        samples = torch.zeros(self.batch_size,self.latent_size)\n",
    "        for index in range(len(indices)):\n",
    "            samples[index] = self.data[indices[index]]\n",
    "        return samples\n",
    "    def double(self):\n",
    "        self.data = torch.cat([self.data,self.data],dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0555511c-5b76-4c6c-80c3-c3e8091ad963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_latent_data(dataset_directory, extractor, prefix = \"/hpc/group/vossenlab/mfm45/.dgl/\", split = 0.8, max_events = 140000, num_samples = 250, mode = \"default\",shuffle = True, double = False):\n",
    "    val_split = (1 - split) / 2\n",
    "    if(mode == \"test\"):\n",
    "        data_range = range(int(split*max_events),int((val_split + split)*max_events))\n",
    "    elif(mode == \"train\"):\n",
    "        data_range = range(0, int(split*max_events))\n",
    "    elif(mode == \"val\"):\n",
    "        data_range = range(int((val_split + split)*max_events),max_events)\n",
    "    elif(mode == \"default\"):\n",
    "        print(f\"No mode given, defaulting to training\\n\")\n",
    "        data_range = range(0, int(split*max_events))\n",
    "    else:\n",
    "        raise Exception(\"Invalid mode: {mode}\\nPlease use either \\\"train,\\\" or \\\"test\\\" \", mode)\n",
    "    dataset = GraphDataset(prefix+dataset_directory)\n",
    "    dataset.load()\n",
    "    if(shuffle):\n",
    "        dataset.shuffle()\n",
    "    dataset = Subset(dataset,data_range)\n",
    "    dgl_batch = dgl.batch(dataset.dataset.graphs[dataset.indices.start:dataset.indices.stop])\n",
    "    labels = dataset.dataset.labels[dataset.indices.start:dataset.indices.stop,0].clone().detach().float().view(-1, 1)\n",
    "    mass = dataset.dataset.labels[dataset.indices.start:dataset.indices.stop,1].clone().detach().float()\n",
    "    dgl_batch = dgl_batch.to(device)\n",
    "    labels = labels.to(device)\n",
    "    latent = extractor.get_latent_repr(dgl_batch).detach().cpu()\n",
    "    latent_obj = Latent_data(latent,labels, double = double)\n",
    "    latent_obj.set_batch_size(num_samples)\n",
    "    latent_obj.set_mass(mass)\n",
    "    return latent_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ade36de-98ea-48e7-8ca1-a8e60ffffd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No mode given, defaulting to training\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_samples = 100\n",
    "training_data_DATA = create_latent_data(DATAdataset, extractor,num_samples = num_samples, max_events = DATA_max_events, double = True)\n",
    "# training_data_MC = create_latent_data(MCdataset, extractor,num_samples = num_samples, max_events = MC_max_events, double = True)\n",
    "\n",
    "# testing_data_DATA = create_latent_data(DATAdataset, extractor, mode = \"test\",num_samples = num_samples, max_events = DATA_max_events, double = True)\n",
    "# testing_data_MC = create_latent_data(MCdataset, extractor, mode = \"test\",num_samples = num_samples, max_events = MC_max_events, double = True)\n",
    "\n",
    "# val_data_DATA = create_latent_data(DATAdataset, extractor, mode = \"val\",num_samples = num_samples, max_events = DATA_max_events, double = True)\n",
    "# val_data_MC = create_latent_data(MCdataset, extractor, mode = \"val\",num_samples = num_samples, max_events = MC_max_events, double = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be5c71b1-4d80-40db-aa88-6cda0d09acd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([199272, 142])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_DATA.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e5fc1f7-fe14-490a-ab76-1ee2d3035189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 142])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_DATA.sample().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09eab31a-0e43-4e18-b436-d17ab8dbbf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_training_DATA = Latent_data(torch.clone(training_data_DATA.data), torch.clone(training_data_DATA.labels), sidebands = True, double = True)\n",
    "sb_training_DATA.set_mass(torch.clone(training_data_DATA.mass))\n",
    "sb_training_DATA.get_sidebands()\n",
    "sb_training_DATA.set_batch_size(100)\n",
    "\n",
    "sb_training_MC = Latent_data(torch.clone(training_data_MC.data), torch.clone(training_data_MC.labels), sidebands = True, double = True)\n",
    "sb_training_MC.set_mass(torch.clone(training_data_MC.mass))\n",
    "sb_training_MC.get_sidebands()\n",
    "sb_training_MC.set_batch_size(100)\n",
    "\n",
    "\n",
    "sb_testing_DATA = Latent_data(torch.clone(testing_data_DATA.data), torch.clone(testing_data_DATA.labels), sidebands = True, double = True)\n",
    "sb_testing_DATA.set_mass(torch.clone(testing_data_DATA.mass))\n",
    "sb_testing_DATA.get_sidebands()\n",
    "sb_testing_DATA.set_batch_size(100)\n",
    "\n",
    "sb_testing_MC = Latent_data(torch.clone(testing_data_MC.data), torch.clone(testing_data_MC.labels), sidebands = True, double = True)\n",
    "sb_testing_MC.set_mass(torch.clone(testing_data_MC.mass))\n",
    "sb_testing_MC.get_sidebands()\n",
    "sb_testing_MC.set_batch_size(100)\n",
    "\n",
    "sb_val_DATA = Latent_data(torch.clone(val_data_DATA.data), torch.clone(val_data_DATA.labels), sidebands = True, double = True)\n",
    "sb_val_DATA.set_mass(torch.clone(val_data_DATA.mass))\n",
    "sb_val_DATA.get_sidebands()\n",
    "sb_val_DATA.set_batch_size(100)\n",
    "\n",
    "sb_val_MC = Latent_data(torch.clone(val_data_MC.data), torch.clone(val_data_MC.labels), sidebands = True, double = True)\n",
    "sb_val_MC.set_mass(torch.clone(val_data_MC.mass))\n",
    "sb_val_MC.get_sidebands()\n",
    "sb_val_MC.set_batch_size(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "985cd965-dcba-4e55-b37e-728155876507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SETTING UP MC MODEL\n",
    "\n",
    "# masked_affine_flows_train_MC = get_masked_affine(latent_dim = 142,num_layers = 40,alternate_mask = False)\n",
    "# distribution_MC = nf.distributions.DiagGaussian(training_data_MC.num_sample_features, trainable = False)\n",
    "# masked_affine_model_MC = nf.NormalizingFlow(q0=distribution_MC, flows=masked_affine_flows_train_MC)\n",
    "# MC_model = masked_affine_model_MC.to(device)\n",
    "\n",
    "# SETTING UP DATA MODEL\n",
    "\n",
    "masked_affine_flows_train_DATA = get_masked_affine(latent_dim = 142,num_layers = 40,alternate_mask = False)\n",
    "distribution_DATA = nf.distributions.DiagGaussian(training_data_DATA.num_sample_features, trainable = False)\n",
    "masked_affine_model_DATA = nf.NormalizingFlow(q0=distribution_DATA, flows=masked_affine_flows_train_DATA)\n",
    "DATA_model = masked_affine_model_DATA.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e270e1e4-478d-4a9e-b46d-41d4b470f296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # TRAINING MC\n",
    "# loss_hist_MC, val_hist_MC, full_loss_hist_MC, full_val_hist_MC = train(sb_training_MC, MC_model, val = True, val_data = sb_val_MC, num_epochs = 6, compact_num = 20)\n",
    "# plot_loss(loss_hist_MC, label = \"MC loss\",plot_val = True, val_loss_hist = val_hist_MC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12193fc5-8010-4ca0-820c-0b9a403bbe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_affine_flows_train_MC = get_masked_affine(latent_dim = 142,num_layers = 40,alternate_mask = False)\n",
    "distribution_MC = nf.distributions.DiagGaussian(training_data_MC.num_sample_features, trainable = False)\n",
    "masked_affine_model_MC = nf.NormalizingFlow(q0=distribution_MC, flows=masked_affine_flows_train_MC)\n",
    "masked_affine_model_MC.load(\"models/NF_MC/MC_2023_07_19_sidebands_double/epochs_6_layers_40.pth\")\n",
    "MC_model = masked_affine_model_MC.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de5608a8-45c5-4351-b09a-7d221a40bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC_model.save(\"models/NF_MC/MC_2023_07_19_sidebands_double/epochs_6_layers_40.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4651703b-fad2-425e-b5f3-6f2452c82565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_affine_flows_train_DATA = get_masked_affine(40)\n",
    "# distribution_DATA = nf.distributions.DiagGaussian(training_data_DATA.latent_size, trainable = False)\n",
    "# masked_affine_model_DATA = nf.NormalizingFlow(q0=distribution_DATA, flows=masked_affine_flows_train_DATA)\n",
    "# masked_affine_model_DATA.load(\"models/NF_DATA/DATA_sidebands_july_10.pth\")\n",
    "# DATA_model = masked_affine_model_DATA.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe296596-06aa-4c75-b83b-5b4de484d5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1566 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (142) must match the size of tensor b (284) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TRAINING DATA\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loss_hist_DATA, val_hist_DATA, full_loss_hist_DATA, full_val_hist_DATA \u001b[38;5;241m=\u001b[39m train(sb_training_DATA, DATA_model, val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, val_data \u001b[38;5;241m=\u001b[39m sb_val_DATA, num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m, compact_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m      3\u001b[0m plot_loss(loss_hist_DATA, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, plot_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, val_loss_hist \u001b[38;5;241m=\u001b[39m val_hist_DATA)\n",
      "File \u001b[0;32m/hpc/group/vossenlab/rck32/Lambda-GNNs/NF_utils.py:347\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(in_data, model, val, val_data, num_epochs, compact_num, distorted, show_progress, lr)\u001b[0m\n\u001b[1;32m    345\u001b[0m     samples \u001b[38;5;241m=\u001b[39m in_data\u001b[38;5;241m.\u001b[39msample(iteration \u001b[38;5;241m=\u001b[39m it)\n\u001b[1;32m    346\u001b[0m samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 347\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward_kld(samples)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# Do backprop and optimizer step\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m~\u001b[39m(torch\u001b[38;5;241m.\u001b[39misnan(loss) \u001b[38;5;241m|\u001b[39m torch\u001b[38;5;241m.\u001b[39misinf(loss)):\n",
      "File \u001b[0;32m/hpc/group/vossenlab/rck32/miniconda3/envs/venv/lib/python3.11/site-packages/normflows/core.py:99\u001b[0m, in \u001b[0;36mNormalizingFlow.forward_kld\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m z \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflows) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 99\u001b[0m     z, log_det \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflows[i]\u001b[38;5;241m.\u001b[39minverse(z)\n\u001b[1;32m    100\u001b[0m     log_q \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m log_det\n\u001b[1;32m    101\u001b[0m log_q \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq0\u001b[38;5;241m.\u001b[39mlog_prob(z)\n",
      "File \u001b[0;32m/hpc/group/vossenlab/rck32/miniconda3/envs/venv/lib/python3.11/site-packages/normflows/flows/affine/coupling.py:221\u001b[0m, in \u001b[0;36mMaskedAffineFlow.inverse\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minverse\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m--> 221\u001b[0m     z_masked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m*\u001b[39m z\n\u001b[1;32m    222\u001b[0m     scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms(z_masked)\n\u001b[1;32m    223\u001b[0m     nan \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mnan, dtype\u001b[38;5;241m=\u001b[39mz\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mz\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (142) must match the size of tensor b (284) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# TRAINING DATA\n",
    "loss_hist_DATA, val_hist_DATA, full_loss_hist_DATA, full_val_hist_DATA = train(sb_training_DATA, DATA_model, val = True, val_data = sb_val_DATA, num_epochs = 6, compact_num = 20)\n",
    "plot_loss(loss_hist_DATA, label = \"DATA loss\", plot_val = True, val_loss_hist = val_hist_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba1b7b-5c00-42b2-a2cf-68ac3ae324bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_model.save(\"models/NF_DATA/DATA_2023_07_19_sidebands_double/epochs_6_layers_40.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf242d-324f-42b2-bea2-ef495d4c407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing MC\n",
    "test(sb_testing_MC, MC_model, data_type = \"MC\")\n",
    "# Testing DATA\n",
    "test(sb_testing_DATA, DATA_model, data_type = \"DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59adb13f-d059-4256-9b0b-1f0d719d2371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5f234-bc0a-44dc-b599-b226f928211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_latent_MC = transform(testing_data_MC, MC_model)\n",
    "transformed_latent_DATA = transform(testing_data_DATA, DATA_model)\n",
    "\n",
    "transformed_latent_train_MC = transform(training_data_MC, MC_model)\n",
    "transformed_latent_train_DATA = transform(training_data_DATA, DATA_model)\n",
    "transformed_latent_val_MC = transform(val_data_MC, MC_model)\n",
    "transformed_latent_val_DATA = transform(val_data_DATA, DATA_model)\n",
    "\n",
    "# From above plotting: <transformed_latent_DATA> is the tensor with normalized DATA\n",
    "# Now need to transform it back to MC version of latent space\n",
    "transformed_latent_DATA_obj = Latent_data(transformed_latent_DATA,testing_data_DATA.labels)\n",
    "transformed_latent_DATA_obj.set_batch_size(num_samples)\n",
    "transformed_latent_MC_obj = Latent_data(transformed_latent_MC,testing_data_MC.labels)\n",
    "transformed_latent_MC_obj.set_batch_size(num_samples)\n",
    "\n",
    "transformed_latent_train_DATA_obj = Latent_data(transformed_latent_train_DATA,training_data_DATA.labels)\n",
    "transformed_latent_train_DATA_obj.set_batch_size(num_samples)\n",
    "transformed_latent_train_MC_obj = Latent_data(transformed_latent_train_MC,training_data_MC.labels)\n",
    "transformed_latent_train_MC_obj.set_batch_size(num_samples)\n",
    "\n",
    "transformed_latent_val_DATA_obj = Latent_data(transformed_latent_val_DATA,val_data_DATA.labels)\n",
    "transformed_latent_val_MC_obj = Latent_data(transformed_latent_val_MC,val_data_MC.labels)\n",
    "\n",
    "full_pass_DATA = transform(transformed_latent_DATA_obj, MC_model, reverse = False)\n",
    "full_pass_DATA_obj = Latent_data(full_pass_DATA, testing_data_DATA.labels)\n",
    "full_pass_DATA_obj.set_batch_size(num_samples)\n",
    "\n",
    "full_pass_train_DATA = transform(transformed_latent_train_DATA_obj, MC_model, reverse = False)\n",
    "full_pass_train_DATA_obj = Latent_data(full_pass_train_DATA, training_data_DATA.labels)\n",
    "full_pass_train_DATA_obj.set_batch_size(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2845795-56b4-4da2-a982-f7f02d9236fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fpd = torch.clone(full_pass_train_DATA)\n",
    "plot_train_data = torch.clone(training_data_DATA.data)\n",
    "for i in range(len(plot_fpd)):\n",
    "    for j in range(71):\n",
    "        if(np.isnan(plot_fpd[i,j])):\n",
    "            plot_fpd[i,0] = 99999\n",
    "            plot_train_data[i,0] = 99999\n",
    "            break\n",
    "plot_fpd = plot_fpd[plot_fpd[:,0] != 99999]\n",
    "plot_train_data = plot_train_data[plot_train_data[:,0] != 99999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb403c-ec13-41aa-8a4b-6738681e7d8d",
   "metadata": {},
   "source": [
    "## Smearing Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb87b87-cfbf-417f-bd65-16711e268291",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax11,ax12,ax13),(ax21,ax22,ax23),(ax31,ax32,ax33)) = plt.subplots(3,3,figsize = (12,12))\n",
    "fig.suptitle(\"NF Smearing Matrices\")\n",
    "axlist = [ax11,ax12,ax13,ax21,ax22,ax23,ax31,ax32,ax33]\n",
    "index_list = [0,1,2,9,10,15,16,17,18]\n",
    "for i in range(9):\n",
    "    x = torch.Tensor.numpy(plot_train_data[:,index_list[i]])\n",
    "    y = torch.Tensor.numpy(plot_fpd[:,index_list[i]])\n",
    "    axlist[i].hist2d(x,y,bins = 100, cmap = plt.cm.jet)\n",
    "plt.show()\n",
    "# plt.savefig(\"plots/NF_sidebands/smear_test2.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736e60a8-a31c-4ab8-ba68-cc8e52e9ec85",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25e85f-45d3-4b3c-a9d7-50d00bc31bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NFClassifier(num_layers = 10).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "num_epochs_classifier = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d960c-d345-43a5-9422-a0d0633d63f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist, val_loss_hist = train_classifier(training_data_MC, classifier, criterion, optimizer, val = True, val_data = val_data_MC, num_epochs = num_epochs_classifier)\n",
    "\n",
    "plot_loss(loss_hist, plot_val =True, val_loss_hist = val_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa2524-407d-4684-b110-f5c71f2a57eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier_MC(testing_data_MC,classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bcd7e4-9e6f-4f47-80d2-a825e20b4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier_data_DEBUG(test_data, classifier):\n",
    "    outputs_data = torch.empty(test_data.num_events,2)\n",
    "    #Converting normalized DATA to classifier output\n",
    "    with tqdm(total=test_data.max_iter, position=0, leave=True) as pbar:\n",
    "        for it in tqdm(range(test_data.max_iter), position = 0, leave=True):\n",
    "            #randomly sample the latent space\n",
    "            samples, labels = test_data.sample(iteration = it, _give_labels = True)\n",
    "            samples = samples.to(device)\n",
    "            # forward + backward + optimize\n",
    "            output_batch = classifier(samples)\n",
    "            for i in range(test_data.batch_size):\n",
    "                outputs_data[it*test_data.batch_size + i] = output_batch[i]\n",
    "    probs_data = torch.softmax(outputs_data, 1)\n",
    "    return probs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5bfd3-2198-49ec-b89a-abddf6458691",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_DATA_test = test_classifier_data_DEBUG(full_pass_DATA_obj, classifier)\n",
    "probs_DATA_fp = test_classifier_data_DEBUG(full_pass_train_DATA_obj, classifier)\n",
    "probs_DATA = test_classifier_data_DEBUG(training_data_DATA, classifier)\n",
    "probs_MC = test_classifier_data_DEBUG(training_data_MC, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c73abc-74c4-4412-8968-46b94631cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(probs_DATA_fp[:,1].detach().to(\"cpu\"),color = \"blue\", alpha = 0.5, bins = 100,density = True, label = \"full pass\");\n",
    "plt.hist(probs_MC[:,1].detach().to(\"cpu\"), color = \"orange\", alpha = 0.5, bins = 100,density = True, label = \"latent MC\");\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed44db-7118-44d6-b346-4d5cae90aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_hist = torch.empty(10,3)\n",
    "for i in range(10):\n",
    "    roc_cut = i / 10\n",
    "    probs_data = probs_DATA_fp\n",
    "    if(roc_cut == 0):\n",
    "        argmax_Y = torch.max(probs_data, 1)[1]\n",
    "    else:\n",
    "        argmax_Y = torch.tensor([1 if el>roc_cut else 0 for el in probs_data[:,1]],dtype=torch.long)\n",
    "\n",
    "    masked_mass = argmax_Y * training_data_DATA.mass\n",
    "    #     signal_mass = argmax_Y * testing_data_DATA.mass\n",
    "    signal_mass = np.array([])\n",
    "    for j in range(masked_mass.size()[0]):\n",
    "        if(masked_mass[j] != 0):\n",
    "            signal_mass = np.append(signal_mass, masked_mass[j])\n",
    "    # Define fit function\n",
    "\n",
    "\n",
    "    low_high = (1.08,1.24)\n",
    "    bins = 100\n",
    "\n",
    "    hdata = np.histogram(signal_mass, range=low_high, bins=bins, density=False);\n",
    "#     hdata = plt.hist(signal_mass, color='tab:orange', alpha=0.5, range=low_high, bins=bins, histtype='stepfilled', density=False, label='signal');\n",
    "\n",
    "    N, beta, m, loc, scale, A, B, C = 10, 1, 1.112, 1.115, 0.008, np.average(hdata[0][-10:-1]), 37, 1.24\n",
    "    d_N, d_beta, d_m, d_loc, d_scale, d_A, d_B, d_C = N/0.01, beta/0.1, m/0.1, loc/0.1, scale/0.01, A/10, B/0.1, C/1\n",
    "    parsMin = [N-d_N, beta-d_beta, m-d_m, loc-d_loc, scale-d_scale, B-d_B]\n",
    "    parsMax = [N+d_N, beta+d_beta, m+d_m, loc+d_loc, scale+d_scale, B+d_B]\n",
    "\n",
    "    def func(x, N, beta, m, loc, scale, B, A=A, C=C):\n",
    "        return N*crystalball.pdf(-x, beta, m, -loc, scale) + A*(1 - B*(x - C)**2)\n",
    "\n",
    "    def sig(x, N, beta, m, loc, scale):\n",
    "        return N*crystalball.pdf(-x, beta, m, -loc, scale)\n",
    "\n",
    "    def bg(x, B, A=A, C=C):\n",
    "        return A*(1 - B*(x - C)**2)\n",
    "\n",
    "    optParams, pcov = opt.curve_fit(func, hdata[1][:-1], hdata[0], method='trf', bounds=(parsMin,parsMax))\n",
    "\n",
    "    x = np.linspace(low_high[0],low_high[1],bins)\n",
    "    y = hdata[0]\n",
    "\n",
    "#     plt.plot(x, func(x, *optParams), color='r')\n",
    "#     plt.plot(x, sig(x, *optParams[0:5]), color='tab:purple')\n",
    "#     plt.plot(x, bg(x, *optParams[5:]), color='b')\n",
    "\n",
    "    bghist = np.histogram(x, weights=y-bg(x, *optParams[5:]), bins=bins, range=low_high);\n",
    "#     bghist = plt.hist(x, weights=y-bg(x, *optParams[5:]), bins=bins, range=low_high, histtype='step', alpha=0.5, color='b');\n",
    "    # plt.savefig(\"plots/bghist_0_07_diff.jpeg\")\n",
    "\n",
    "    r = np.divide(y - func(x, *optParams),np.sqrt([el if el>0 else 1 for el in func(x, *optParams)]))\n",
    "    chi2 = np.sum(np.square(r))\n",
    "    chi2ndf = chi2/len(optParams)\n",
    "\n",
    "    # Get S and N before and after? #DEBUGGING: ADDED\n",
    "    import scipy.integrate as integrate\n",
    "    mu      = optParams[3]\n",
    "    sigma   = optParams[4]\n",
    "    mmin    = mu - 2*sigma\n",
    "    mmax    = mu + 2*sigma\n",
    "\n",
    "\n",
    "    binwidth = (low_high[1]-low_high[0])/bins#KEEP!!!\n",
    "\n",
    "    bin1 = int((mmin-low_high[0])/binwidth)\n",
    "    bin2 = int((mmax-low_high[0])/binwidth)\n",
    "\n",
    "    integral_bghist = sum(bghist[0][bin1:bin2])\n",
    "\n",
    "    integral_tothist = sum(hdata[0][bin1:bin2])\n",
    "    # try:\n",
    "    fom = integral_bghist/np.sqrt(integral_tothist)\n",
    "    purity =(integral_bghist)/integral_tothist\n",
    "    cut_hist[i] = torch.tensor([roc_cut, fom, purity])\n",
    "    # except Exception as inst:\n",
    "    #     print(f\"Caught {inst} | skipping cut #{i} = {roc_cut}\")\n",
    "#     print(f\"roc_cut = {roc_cut} | FOM: {fom} | purity: {purity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3a733-f0c4-4269-8d92-f3ab6be352b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('cut')\n",
    "ax1.scatter(cut_hist[:,0], cut_hist[:,1], label = \"FOM\", color = \"tab:red\",marker = '+', s = 10)\n",
    "ax1.set_ylabel('FOM', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_ylim([0, 50])\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('purity', color=color)  # we already handled the x-label with ax1\n",
    "ax2.scatter(cut_hist[:,0], cut_hist[:,2], label = \"purity\", color = \"tab:blue\",marker = '+',s = 10)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_ylim([0, 1])\n",
    "fig.legend()\n",
    "fig.text(0.6,0.2,\"DATA\")\n",
    "fig.text(0.6,0.15,f\"{len(probs_data)} events\")\n",
    "# fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()\n",
    "# fig.savefig(\"plots/FOMpure/DATA_fp.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d1144-7064-49a8-a69b-73a3f0377838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
