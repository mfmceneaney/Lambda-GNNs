{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e16a5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST IPYNB FOR DAGNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa7ec1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "# ML Imports\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# DGL Graph Learning Imports\n",
    "from dgl import save_graphs, load_graphs, batch\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl.data.utils import save_info, load_info\n",
    "\n",
    "# PyTorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Functional\n",
    "import torch.optim as optim\n",
    "from dgl.data.utils import Subset\n",
    "\n",
    "# PyTorch Ignite Imports\n",
    "from ignite.engine import Engine, Events, EventEnum, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.contrib.metrics import ROC_AUC, RocCurve\n",
    "from ignite.contrib.handlers.tensorboard_logger import *\n",
    "from ignite.handlers import global_step_from_engine, EarlyStopping\n",
    "\n",
    "# Miscellaneous Imports\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5788a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------#\n",
    "# Model Definitions\n",
    "# - ApplyNodeFunc\n",
    "# - MLP\n",
    "# - GIN\n",
    "#--------------------------------------------------#\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# DGL Graph Learning Imports\n",
    "import dgl\n",
    "from dgl.nn.pytorch.conv import GINConv\n",
    "from dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling\n",
    "\n",
    "# PyTorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Functional\n",
    "\n",
    "# GIN ARCHITECTURE\n",
    "\n",
    "\"\"\"\n",
    "How Powerful are Graph Neural Networks\n",
    "https://arxiv.org/abs/1810.00826\n",
    "https://openreview.net/forum?id=ryGs6iA5Km\n",
    "Author's implementation: https://github.com/weihua916/powerful-gnns\n",
    "\"\"\"\n",
    "\n",
    "class ApplyNodeFunc(nn.Module):\n",
    "    \"\"\"Update the node feature hv with MLP, BN and ReLU.\"\"\"\n",
    "    def __init__(self, mlp):\n",
    "        super(ApplyNodeFunc, self).__init__()\n",
    "        self.mlp = mlp\n",
    "        self.bn = nn.BatchNorm1d(self.mlp.output_dim)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.mlp(h)\n",
    "        h = self.bn(h)\n",
    "        h = Functional.relu(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP with linear output\"\"\"\n",
    "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"MLP layers construction\n",
    "\n",
    "        Paramters\n",
    "        ---------\n",
    "        num_layers: int\n",
    "            The number of linear layers\n",
    "        input_dim: int\n",
    "            The dimensionality of input features\n",
    "        hidden_dim: int\n",
    "            The dimensionality of hidden units at ALL layers\n",
    "        output_dim: int\n",
    "            The number of classes for prediction\n",
    "\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_or_not = True  # default is linear model\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        if num_layers < 1:\n",
    "            raise ValueError(\"number of layers should be positive!\")\n",
    "        elif num_layers == 1:\n",
    "            # Linear model\n",
    "            self.linear = nn.Linear(input_dim, output_dim)\n",
    "        else:\n",
    "            # Multi-layer model\n",
    "            self.linear_or_not = False\n",
    "            self.linears = torch.nn.ModuleList()\n",
    "            self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "            self.linears.append(nn.Linear(input_dim, hidden_dim))\n",
    "            for layer in range(num_layers - 2):\n",
    "                self.linears.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.linears.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "            for layer in range(num_layers - 1):\n",
    "                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.linear_or_not:\n",
    "            # If linear model\n",
    "            return self.linear(x)\n",
    "        else:\n",
    "            # If MLP\n",
    "            h = x\n",
    "            for i in range(self.num_layers - 1):\n",
    "                h = Functional.relu(self.batch_norms[i](self.linears[i](h)))\n",
    "            return self.linears[-1](h)\n",
    "\n",
    "\n",
    "class GIN(nn.Module):\n",
    "    \"\"\"GIN model\"\"\"\n",
    "    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim,\n",
    "                 output_dim, final_dropout, learn_eps, graph_pooling_type,\n",
    "                 neighbor_pooling_type):\n",
    "        \"\"\"model parameters setting\n",
    "\n",
    "        Paramters\n",
    "        ---------\n",
    "        num_layers: int\n",
    "            The number of linear layers in the neural network\n",
    "        num_mlp_layers: int\n",
    "            The number of linear layers in mlps\n",
    "        input_dim: int\n",
    "            The dimensionality of input features\n",
    "        hidden_dim: int\n",
    "            The dimensionality of hidden units at ALL layers\n",
    "        output_dim: int\n",
    "            The number of classes for prediction\n",
    "        final_dropout: float\n",
    "            dropout ratio on the final linear layer\n",
    "        learn_eps: boolean\n",
    "            If True, learn epsilon to distinguish center nodes from neighbors\n",
    "            If False, aggregate neighbors and center nodes altogether.\n",
    "        neighbor_pooling_type: str\n",
    "            how to aggregate neighbors (sum, mean, or max)\n",
    "        graph_pooling_type: str\n",
    "            how to aggregate entire nodes in a graph (sum, mean or max)\n",
    "\n",
    "        \"\"\"\n",
    "        super(GIN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.learn_eps = learn_eps\n",
    "\n",
    "        # List of MLPs\n",
    "        self.ginlayers = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            if layer == 0:\n",
    "                mlp = MLP(num_mlp_layers, input_dim, hidden_dim, hidden_dim)\n",
    "            else:\n",
    "                mlp = MLP(num_mlp_layers, hidden_dim, hidden_dim, hidden_dim)\n",
    "\n",
    "            self.ginlayers.append(\n",
    "                GINConv(ApplyNodeFunc(mlp), neighbor_pooling_type, 0, self.learn_eps))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Linear function for graph poolings of output of each layer\n",
    "        # which maps the output of different layers into a prediction score\n",
    "        self.linears_prediction = torch.nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            if layer == 0:\n",
    "                self.linears_prediction.append(\n",
    "                    nn.Linear(input_dim, output_dim))\n",
    "            else:\n",
    "                self.linears_prediction.append(\n",
    "                    nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.drop = nn.Dropout(final_dropout)\n",
    "\n",
    "        if graph_pooling_type == 'sum':\n",
    "            self.pool = SumPooling()\n",
    "        elif graph_pooling_type == 'mean':\n",
    "            self.pool = AvgPooling()\n",
    "        elif graph_pooling_type == 'max':\n",
    "            self.pool = MaxPooling()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, g, key='data'):\n",
    "        # list of hidden representation at each layer (including input)\n",
    "        h = g.ndata[key].float()\n",
    "        hidden_rep = [h]\n",
    "\n",
    "        for i in range(self.num_layers - 1):\n",
    "            h = self.ginlayers[i](g, h)\n",
    "            h = self.batch_norms[i](h)\n",
    "            h = Functional.relu(h)\n",
    "            hidden_rep.append(h)\n",
    "\n",
    "        score_over_layer = 0\n",
    "\n",
    "        # perform pooling over all nodes in each graph in every layer\n",
    "        for i, h in enumerate(hidden_rep):\n",
    "            pooled_h = self.pool(g, h)\n",
    "            score_over_layer += self.drop(self.linears_prediction[i](pooled_h))\n",
    "\n",
    "        return score_over_layer\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\"Name of model.\"\"\"\n",
    "        return \"GIN\"\n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "        Classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=512, num_classes=10):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, h):\n",
    "        c = self.layer(h)\n",
    "        return c\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "        Simple Discriminator w/ MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=512, num_classes=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, num_classes),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, h):\n",
    "        y = self.layer(h)\n",
    "        return y\n",
    "    \n",
    "# Define dataset class\n",
    "class GraphDataset(DGLDataset):\n",
    "    _url = None\n",
    "    _sha1_str = None\n",
    "    mode = \"mode\"\n",
    "    num_classes = 2\n",
    "    dataset = None\n",
    "\n",
    "    def __init__(self, name, dataset=None, raw_dir=None, force_reload=False, verbose=False, num_classes=2):\n",
    "        self.dataset = dataset\n",
    "        super(GraphDataset, self).__init__(name=name,\n",
    "                                          url=self._url,\n",
    "                                          raw_dir=raw_dir,\n",
    "                                          force_reload=force_reload,\n",
    "                                          verbose=verbose\n",
    "                                          )\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def process(self):\n",
    "        mat_path = os.path.join(self.raw_path,self.mode+'_dgl_graph.bin')\n",
    "        # process data to a list of graphs and a list of labels\n",
    "        if self.dataset != None:\n",
    "            self.graphs, self.labels = self.dataset[\"data\"], torch.LongTensor(self.dataset[\"target\"])\n",
    "        else:\n",
    "            self.graphs, self.labels = load_graphs(mat_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Get graph and label by index\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Item index\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (dgl.DGLGraph, Tensor)\n",
    "        \"\"\"\n",
    "        return self.graphs[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of graphs in the dataset\"\"\"\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def save(self):\n",
    "        # save graphs and labels\n",
    "        graph_path = os.path.join(self.save_path, self.mode + '_dgl_graph.bin')\n",
    "        save_graphs(graph_path, self.graphs, {'labels': self.labels})\n",
    "        # save other information in python dict\n",
    "        info_path = os.path.join(self.save_path, self.mode + '_info.pkl')\n",
    "        save_info(info_path, {'num_classes': self.num_classes})\n",
    "    \n",
    "    def load(self):\n",
    "        # load processed data from directory `self.save_path`\n",
    "        graph_path = os.path.join(self.save_path, self.mode + '_dgl_graph.bin')\n",
    "        self.graphs, label_dict = load_graphs(graph_path)\n",
    "        self.labels = label_dict['labels']\n",
    "        info_path = os.path.join(self.save_path, self.mode + '_info.pkl')\n",
    "        self.num_classes = load_info(info_path)['num_classes']\n",
    "\n",
    "    def has_cache(self):\n",
    "        # check whether there are processed data in `self.save_path`\n",
    "        graph_path = os.path.join(self.save_path, self.mode + '_dgl_graph.bin')\n",
    "        info_path = os.path.join(self.save_path, self.mode + '_info.pkl')\n",
    "        return os.path.exists(graph_path) and os.path.exists(info_path)\n",
    "    \n",
    "    @property\n",
    "    def num_labels(self):\n",
    "        \"\"\"Number of labels for each graph, i.e. number of prediction tasks.\"\"\"\n",
    "        return 2\n",
    "    \n",
    "\n",
    "def load_graph_dataset(dataset=\"\",prefix=\"\",split=0.75,max_events=1e5,batch_size=1024,drop_last=False,shuffle=True,num_workers=0,pin_memory=True, verbose=True):\n",
    "\n",
    "    # Load training data\n",
    "    train_dataset = GraphDataset(prefix+dataset) # Make sure this is copied into ~/.dgl folder\n",
    "    train_dataset.load()\n",
    "    num_labels = train_dataset.num_labels\n",
    "    node_feature_dim = train_dataset.graphs[0].ndata[\"data\"].shape[-1]\n",
    "    index = int(min(len(train_dataset),max_events)*split)\n",
    "    train_dataset = Subset(train_dataset,range(index))\n",
    "\n",
    "    # Create training dataloader\n",
    "    train_loader = GraphDataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=drop_last,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        num_workers=num_workers)\n",
    "\n",
    "    # Load validation data\n",
    "    val_dataset = GraphDataset(prefix+dataset) # Make sure this is copied into ~/.dgl folder\n",
    "    val_dataset.load()\n",
    "    val_dataset = Subset(val_dataset,range(index,len(val_dataset)))\n",
    "\n",
    "    # Create testing dataloader\n",
    "    val_loader = GraphDataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=drop_last,\n",
    "        shuffle=False,\n",
    "        pin_memory=pin_memory,\n",
    "        num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, num_labels, node_feature_dim   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2e56b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes =  2\n",
      "node_feature_dim =  6\n",
      "num_classes2 =  2\n",
      "node_feature_dim2 =  8\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Training routine #TODO: Put this in train_step and val_step functions.\n",
    "\n",
    "device = 'cpu'\n",
    "dataset = 'gangelmc_100k_2021-07-28_noEtaOldChi2_addKin_train'\n",
    "dataset = 'test_dataset' #DEBUGGING\n",
    "\n",
    "batch_size=1024\n",
    "\n",
    "# Initialize data loaders\n",
    "train_loader, val_loader, num_classes, node_feature_dim = load_graph_dataset(\n",
    "                                                                            dataset=dataset,\n",
    "                                                                            prefix=\"\",\n",
    "                                                                            split=0.75,\n",
    "                                                                            max_events=1e5,\n",
    "                                                                            batch_size=1024,\n",
    "                                                                            drop_last=False,\n",
    "                                                                            shuffle=True,\n",
    "                                                                            num_workers=0,\n",
    "                                                                            pin_memory=True,\n",
    "                                                                            verbose=True\n",
    "                                                                           )\n",
    "# Get domain data loader\n",
    "dataset2 = \"gangelmc_100k_2021-07-28_noEtaOldChi2_addKin_test\"\n",
    "domain_loader, _, num_classes2, node_feature_dim2 = load_graph_dataset(\n",
    "                                                                            dataset=dataset2,\n",
    "                                                                            prefix=\"\",\n",
    "                                                                            split=1.00,\n",
    "                                                                            max_events=1e5,\n",
    "                                                                            batch_size=1024,\n",
    "                                                                            drop_last=False,\n",
    "                                                                            shuffle=True,\n",
    "                                                                            num_workers=0,\n",
    "                                                                            pin_memory=True,\n",
    "                                                                            verbose=True\n",
    "                                                                           )\n",
    "print(\"num_classes = \",num_classes)\n",
    "print(\"node_feature_dim = \",node_feature_dim)\n",
    "print(\"num_classes2 = \",num_classes2)\n",
    "print(\"node_feature_dim2 = \",node_feature_dim2)\n",
    "\n",
    "\n",
    "# Initialize GIN parameters\n",
    "num_layers = 3\n",
    "num_mlp_layers = 3\n",
    "input_dim = node_feature_dim\n",
    "hidden_dim = 64\n",
    "output_dim = 64\n",
    "final_dropout = 0.8\n",
    "learn_eps = False\n",
    "graph_pooling_type = \"max\"\n",
    "neighbor_pooling_type = \"max\"\n",
    "\n",
    "num_domains = 1\n",
    "\n",
    "\n",
    "# Set models\n",
    "F = GIN(num_layers, num_mlp_layers, input_dim, hidden_dim,\n",
    "                 output_dim, final_dropout, learn_eps, graph_pooling_type,\n",
    "                 neighbor_pooling_type).to(device)\n",
    "C = Classifier(input_size=output_dim,num_classes=num_classes).to(device)\n",
    "D = Discriminator(input_size=output_dim,num_classes=num_domains).to(device)\n",
    "\n",
    "# Initialize losses\n",
    "bce = nn.BCELoss()\n",
    "xe  = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize optimizers\n",
    "F_opt = torch.optim.Adam(F.parameters())\n",
    "C_opt = torch.optim.Adam(C.parameters())\n",
    "D_opt = torch.optim.Adam(D.parameters())\n",
    "\n",
    "# Set training parameters\n",
    "max_epoch = 50\n",
    "step = 1\n",
    "n_critic = 1 # for training more k steps about Discriminator\n",
    "n_batches = len(train_loader)//batch_size\n",
    "# lamda = 0.01\n",
    "\n",
    "# Set classification and domain labels\n",
    "D_src = torch.ones(batch_size, 1).to(device) # Discriminator Label to real\n",
    "D_tgt = torch.zeros(batch_size, 1).to(device) # Discriminator Label to fake\n",
    "D_labels = torch.cat([D_src, D_tgt], dim=0)\n",
    "\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aff9ea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes =  2\n",
      "node_feature_dim =  8\n",
      "num_classes2 =  2\n",
      "node_feature_dim2 =  8\n"
     ]
    }
   ],
   "source": [
    "# Get domain data loader\n",
    "# dataset2 = \"gangelmc_100k_2021-07-28_noEtaOldChi2_addKin_test\"\n",
    "# domain_loader, _, num_classes2, node_feature_dim2 = load_graph_dataset(\n",
    "#                                                                             dataset=dataset2,\n",
    "#                                                                             prefix=\"\",\n",
    "#                                                                             split=1.00,\n",
    "#                                                                             max_events=1e5,\n",
    "#                                                                             batch_size=1024,\n",
    "#                                                                             drop_last=False,\n",
    "#                                                                             shuffle=True,\n",
    "#                                                                             num_workers=0,\n",
    "#                                                                             pin_memory=True,\n",
    "#                                                                             verbose=True\n",
    "#                                                                            )\n",
    "print(\"num_classes = \",num_classes)\n",
    "print(\"node_feature_dim = \",node_feature_dim)\n",
    "print(\"num_classes2 = \",num_classes2)\n",
    "print(\"node_feature_dim2 = \",node_feature_dim2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6da2178",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([2048, 1])) that is different to the input size (torch.Size([2045, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-0218d26f158c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mLd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mLd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2882\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2883\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2884\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2885\u001b[0m             \u001b[0;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2886\u001b[0m             \u001b[0;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([2048, 1])) that is different to the input size (torch.Size([2045, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "step=1\n",
    "if n_batches<1: n_batches = 1\n",
    "# Function to decrease lambda with epoch\n",
    "def get_lambda(epoch, max_epoch):\n",
    "    p = epoch / max_epoch\n",
    "    return 2. / (1+np.exp(-10.*p)) - 1.\n",
    "\n",
    "# Function to continuously sample target domain data\n",
    "domain_set = iter(domain_loader)\n",
    "def sample_domain(step, n_batches):\n",
    "    global domain_set\n",
    "    if step % n_batches == 0:\n",
    "        domain_set = iter(domain_loader)\n",
    "    return domain_set.next()\n",
    "\n",
    "# Not really sure yet...\n",
    "ll_c, ll_d = [], []\n",
    "acc_lst = []\n",
    "\n",
    "for epoch in range(1, max_epoch+1):\n",
    "    for idx, (src_images, labels) in enumerate(train_loader):\n",
    "        tgt_images, _ = sample_domain(step, n_batches)\n",
    "        # Training Discriminator\n",
    "        src, labels_, tgt = src_images.to(device), labels.to(device), tgt_images.to(device)\n",
    "        \n",
    "        # Important: since we have kinematics added the labels are actually just first entries.\n",
    "        labels = labels_[:,0].clone().detach().long()\n",
    "#         print(labels_[0,:10])\n",
    "#         print(labels_[1,:10])\n",
    "        \n",
    "        x = dgl.unbatch(src)\n",
    "#       #print(type(x))#DEBUGGING\n",
    "        for el in dgl.unbatch(tgt):\n",
    "            x.append(el)#OLD: torch.cat([src, tgt], dim=0)\n",
    "        x = dgl.batch(x)\n",
    "        h = F(x)\n",
    "        y = D(h.detach())\n",
    "        \n",
    "        Ld = bce(y, D_labels)\n",
    "        D.zero_grad()\n",
    "        Ld.backward()\n",
    "        D_opt.step()\n",
    "        \n",
    "        \n",
    "        c = C(h[:batch_size])\n",
    "        y = D(h)\n",
    "#         print(np.shape(labels))#DEBUGGING\n",
    "        Lc = xe(c, labels)\n",
    "        Ld = bce(y, D_labels)\n",
    "        lamda = 0.1*get_lambda(epoch, max_epoch)\n",
    "        Ltot = Lc -lamda*Ld\n",
    "        \n",
    "        \n",
    "        F.zero_grad()\n",
    "        C.zero_grad()\n",
    "        D.zero_grad()\n",
    "        \n",
    "        Ltot.backward()\n",
    "        \n",
    "        C_opt.step()\n",
    "        F_opt.step()\n",
    "        \n",
    "        if step %  == 0\n",
    "            dt = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "            print('Epoch: {}/{}, Step: {}, D Loss: {:.4f}, C Loss: {:.4f}, lambda: {:.4f} ---- {}'.format(epoch, max_epoch, step, Ld.item(), Lc.item(), lamda, dt))\n",
    "            ll_c.append(Lc)\n",
    "            ll_d.append(Ld)\n",
    "        \n",
    "        if step % 5 == 0:\n",
    "            F.eval()\n",
    "            C.eval()\n",
    "            with torch.no_grad():\n",
    "                corrects = torch.zeros(1).to(device)\n",
    "                for idx, (src, labels) in enumerate(val_loader):\n",
    "                    src, labels = src.to(device), labels.to(device)\n",
    "                    c = C(F(src))\n",
    "                    _, preds = torch.max(c, 1)\n",
    "                    corrects += (preds == labels).sum()\n",
    "                acc = corrects.item() / len(val_loader.dataset)\n",
    "                print('***** Eval Result: {:.4f}, Step: {}'.format(acc, step))\n",
    "                \n",
    "                corrects = torch.zeros(1).to(device)\n",
    "                for idx, (tgt, labels) in enumerate(vdomain_loader):\n",
    "                    tgt, labels = tgt.to(device), labels.to(device)\n",
    "                    c = C(F(tgt))\n",
    "                    _, preds = torch.max(c, 1)\n",
    "                    corrects += (preds == labels).sum()\n",
    "                acc = corrects.item() / len(domain_loader.dataset)\n",
    "                print('***** Test Result: {:.4f}, Step: {}'.format(acc, step))\n",
    "                acc_lst.append(acc)\n",
    "                \n",
    "            F.train()\n",
    "            C.train()\n",
    "        step += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276400ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d50e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader))\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4072426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
